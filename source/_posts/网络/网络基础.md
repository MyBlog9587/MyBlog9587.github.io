---
title: 网络基础
date: 2023-05-23 17:16:55
categories: 
- 网络
tags:
- linux
- 网络
- private
---




# 基本知识

## 子网掩码

第一是网络（network）的概念，直观点说，它表示的是这组 IP 共同的部分，比如在192.168.1.1~192.168.1.255 这个区间里，它们共同的部分是 192.168.1.0。

第二是主机（host）的概念，它表示的是这组 IP 不同的部分，上面的例子中 1~255 就是不同的那些部分，表示有 255 个可用的不同 IP。

计算方式：

​		查看最后一个不可变的位置，就是当前掩码数。当前位置可以通过总长度 - （后面可变位数 + 当前值的位置 ）

​		例如：

​			ipv4：192.168.1.0 取值范围：192.168.1.0～192.168.1.255 掩码表示为 192.168.1.0/24（24表示前24位不可变 192 -- 8位，168 -- 8位，001 -- 8位）

​			ipv6：2001:218:6000::  到 2001:218:6000:3:ffff:ffff:ffff:ffff 的掩码为 2001:218:6000/62（62 = 128-3:ffff:ffff:ffff:ffff， 其中3:ffff:ffff:ffff:ffff 解释为 ffff -- 1111 1111 1111 1111 占16为，:3: -- 0003 -- 0011 占2位，2 + 16*4 = 66， 最后128 -66 = 62）


## 域名系统
全球域名按照从大到小的结构，形成了一棵树状结构。实际访问一个域名时，是从最底层开始写起

<img src="域名级别.png" style="zoom:80%;" >


## OSI 和 TCP/IP 协议栈

- OSI: 七层协议（应用层、表示层、会话层、传输层、网络层、数据链路层、物理层）
- TCP/IP 协议栈： 四层协议（应用层、传输层、网际层、网络接口层）

<img src="网络模型对照.png" style="zoom:50%;" >

![image](https://user-images.githubusercontent.com/32731294/188394903-10e1dea3-5b3b-44c3-81dd-71c05abdf455.png)


## 网络数据包剖析
![image](https://user-images.githubusercontent.com/32731294/188426341-b432664e-be6e-4240-970e-7eccd0d58804.png)



## IP 寻址、子网和 IP 路由
IP 地址有两种变体：IPv4 和 IPv6。
- IPv4 地址长度为 32 位，是最常用的。它们通常表示为 4 个十进制字节（每个 0-255），由点分隔。例如192.168.27.64
- IPv6 地址长度为 128 位，旨在克服 IPv4 地址空间不足的问题。它们通常由 8 组 4 位十六进制数表示。例如1203:8fe0:fe80:b897:8990:8a7c:99bf:323d

IP 地址组通常使用 [CIDR](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing) 表示法表示它由 IP 地址和 IP 地址上的有效位数组成，由/. 例如，192.168.27.0/24表示从192.168.27.0 到 192.168.27.255 的 256 个 IP 地址组

单个 L2 网络中的一组 IP 地址称为**子网**。在子网内，数据包可以在任何一对设备之间作为单个网络跃点发送，仅基于 L2 标头（和页脚）

将数据包发送到单个子网之外需要 L3 路由，每个 L3 网络设备（路由器）负责根据 L3 路由规则决定发送数据包的路径。充当路由器的每个网络设备都有路由，这些路由确定特定 CIDR 的数据包接下来应该发送到哪里。例如，在 Linux 系统中，路由 `10.48.0.128/26 via 10.0.0.12 dev eth0` 表示需要将目的地址为 10.48.0.128/26 的报文通过 eth0 接口路由到下一跳 10.0.0.12


## NAT
网络地址转换 ( NAT) 是当数据包通过执行 NAT 的设备时，将数据包中的 IP 地址映射到不同 IP 地址的过程。根据使用情况，NAT 可以应用于源 IP 地址或目标 IP 地址，或同时应用于这两个地址。

### SNAT(源网络地址转换)
具有私有 IP 地址的设备通过 Internet 与具有公共 IP 地址的设备通信。例如，如果具有私有 IP 地址的设备尝试连接到公共 IP 地址，则位于私有网络边界的路由器通常会使用 SNAT（源网络地址转换）来映射数据包的私有源 IP 地址到路由器自己的公共 IP 地址，然后再将其转发到 Internet。然后，路由器将来自相反方向的响应数据包映射回原始私有 IP 地址，因此数据包在两个方向上端到端流动，源或目的地都不知道映射正在发生。相同的技术通常用于允许连接到覆盖网络的设备与覆盖网络外部的设备连接。

### DNAT（目标网络地址转换）
将传入连接的目标 IP 地址更改为要进行负载平衡的所选设备的 IP 地址。然后，负载均衡器在响应数据包上反转此 NAT，因此源设备或目标设备都不知道映射正在发生。


## MTU
最大传输单位（MTU) 的网络链接是可以通过该网络链接发送的数据包的最大大小。 网络中的所有链路通常都配置相同的 MTU，以减少在数据包穿越网络时对数据包进行分段的需要，这会显着降低网络性能。此外，TCP 尝试学习路径 MTU，并根据网络路径中任何链路的最小 MTU 调整每个网络路径的数据包大小。当应用程序尝试发送的数据超出单个数据包的容量时，TCP 会将数据分成多个 TCP 段，因此不会超过 MTU。




# 套接字地址格式

```c
#include <sys/types.h> 
#include <sys/socket.h>
       
int socket(int domain, int type, int protocol);
```


## 通用套接字地址格式
```c
/* POSIX.1g 规范规定了地址族为 2 字节的值. */
typedef unsigned short int sa_family_t;
/* 描述通用套接字地址 */
struct sockaddr{
  sa_family_t sa_family; /* 地址族. 16-bit*/
  char sa_data[14]; /* 具体的地址值 112-bit */
};
```

第一个字段是地址族，它表示使用什么样的方式对地址进行解释和保存，好比电话簿里的手机格式，或者是固话格式，这两种格式的长度和含义都是不同的。地址族在 glibc 里的定义非常多，常用的有以下几种：
- **AF_LOCAL**：表示的是本地地址，对应的是 Unix 套接字，这种情况一般用于本地 socket 通信，很多情况下也可以写成 AF_UNIX、AF_FILE；
- **AF_INET**：因特网使用的 IPv4 地址；
- **AF_INET6**：因特网使用的 IPv6 地址。

> `AF_` 表示的含义是 Address Family; 也会看到以 `PF_` 表示的宏，比如 PF_INET、PF_INET6 等，实际上 PF_ 的意思是 Protocol Family，也就是协议族的意思.

**用 AF_xxx 这样的值来初始化 socket 地址，用 PF_xxx 这样的值来初始化 socket**. 在 <sys/socket.h> 头文件中可以清晰地看到，这两个值本身就是一一对应

```c
/* 各种地址族的宏定义 */
#define AF_UNSPEC PF_UNSPEC
#define AF_LOCAL PF_LOCAL
#define AF_UNIX PF_UNIX
#define AF_FILE PF_FILE
#define AF_INET PF_INET
#define AF_AX25 PF_AX25
#define AF_IPX PF_IPX
#define AF_APPLETALK PF_APPLETALK
#define AF_NETROM PF_NETROM
#define AF_BRIDGE PF_BRIDGE
#define AF_ATMPVC PF_ATMPVC
#define AF_X25 PF_X25
#define AF_INET6 PF_INET6
```

## IPv4 套接字格式地址
```c
/* IPV4 套接字地址，32bit 值. */
typedef uint32_t in_addr_t;
struct in_addr
{
  in_addr_t s_addr;
};

/* 描述 IPV4 的套接字地址格式 */
struct sockaddr_in
{
  sa_family_t sin_family; /* 16-bit  对于 IPv4 来说这个值就是 AF_INET*/
  in_port_t sin_port; /* 端口口 16-bit*/
  struct in_addr sin_addr; /* Internet address. 32-bit */
  
  /* 这里仅仅用作占位符，不做实际用处 */
  unsigned char sin_zero[8];
};
```

和 sockaddr 一样，都有一个 16-bit 的 sin_family 字段，对于 IPv4 来说这个值就是 AF_INET。
端口号最多是 16-bit，也就是说最大支持 2 的 16 次方，这个数字是 65536，所以我们应该知道支持寻址的端口号最多就是 65535

保留端口在 glibc 定义：
```c
/* Standard well-known ports. */
enum
{
  IPPORT_ECHO = 7, /* Echo service. */
  IPPORT_DISCARD = 9, /* Discard transmissions service. */
  IPPORT_SYSTAT = 11, /* System status service. */
  IPPORT_DAYTIME = 13, /* Time of day service. */
  IPPORT_NETSTAT = 15, /* Network status service. */
  IPPORT_FTP = 21, /* File Transfer Protocol. */
  IPPORT_TELNET = 23, /* Telnet protocol. */
  IPPORT_SMTP = 25, /* Simple Mail Transfer Protocol. */
  IPPORT_TIMESERVER = 37, /* Timeserver service. */
  IPPORT_NAMESERVER = 42, /* Domain Name Service. */
  IPPORT_WHOIS = 43, /* Internet Whois service. */
  IPPORT_MTP = 57,
  
  IPPORT_TFTP = 69, /* Trivial File Transfer Protocol. */
  IPPORT_RJE = 77,
  IPPORT_FINGER = 79, /* Finger service. */
  IPPORT_TTYLINK = 87,
  IPPORT_SUPDUP = 95, /* SUPDUP protocol. */
  IPPORT_EXECSERVER = 512, /* execd service. */
  IPPORT_LOGINSERVER = 513, /* rlogind service. */
  IPPORT_CMDSERVER = 514,
  IPPORT_EFSSERVER = 520,
  /* UDP ports. */
  IPPORT_BIFFUDP = 512,
  IPPORT_WHOSERVER = 513,
  IPPORT_ROUTESERVER = 520,
  /* Ports less than this value are reserved for privileged processes. */
  IPPORT_RESERVED = 1024,
  /* Ports greater this value are reserved for (non-privileged) servers. */
  IPPORT_USERRESERVED = 5000
 }
```

> 一般而言，大于 5000 的端口可以作为自己应用程序的端口使用

实际的 IPv4 地址是一个 32-bit 的字段，可以想象最多支持的地址数就是 2 的 32 次方，大约是 42 亿,这个数字渐渐显得不太够用了，于是大家所熟知的 IPv6 就隆重登场了


## IPv6 套接字地址格式
```c
struct sockaddr_in6
{
  sa_family_t sin6_family; /* 16-bit */
  in_port_t sin6_port; /* 传输端口号 # 16-bit */
  uint32_t sin6_flowinfo; /* IPv6 流控信息 32-bit*/
  struct in6_addr sin6_addr; /* IPv6 地址 128-bit */
  uint32_t sin6_scope_id; /* IPv6 域 ID 32-bit */
};
```

整个结构体长度是 28 个字节, 地址族显然应该是 AF_INET6，端口同 IPv4 地址一样，关键的地址从 32 位升级到 128 位, 解决了寻址数字不够的问题.

本地套接字格式，用来做为本地进程间的通信， 也就是前面提到的 AF_LOCAL。
```c
struct sockaddr_un {
  unsigned short sun_family; /* 固定为 AF_LOCAL */
  char sun_path[108]; /* 路径名 */
};
```

## 几种套接字地址格式比较

IPv4 和 IPv6 套接字地址结构的长度是固定的，而本地地址结构的长度是可变的。

<img src="套接字格式比较.png" style="zoom:80%;" >



## SOCK_PACKET 和 PF_PACKET
在Linux下，有两种方法可以从数据链路层接收数据包: 
- 最初的方法是创建一个 `SOCK_PACKET` 类型的套接字，这种方法更为**广泛，但灵活性较差**。
- 较新的方法是创建一个 `PF_PACKET` 类型的套接字，它**引入了更多的过滤和性能特征**。
 
要做到这两点，我们必须有**足够的权限**（类似于创建一个原始套接字），并且套接字的第三个参数必须是指定以太网帧类型的非零值. 

当使用 `PF_PACKET` 套接字时，套接字的第二个参数可以是 `SOCK_DGRAM`，用于去掉链路层头的 "cooked" 数据包（即去掉MAC header），或 `SOCK_RAW` 用于完整的链路层数据包。 

`SOCK_PACKET` 套接字只返回完整的链路层数据包。例如，要从数据链路接收所有帧

- 形式一：
```c
fd = socket(PF_PACKET, SOCK_RAW, htons(ETH_P_ALL));        /* 较新的系统*/
```
- 形式二：
```c
fd = socket(AF_INET, SOCK_PACKET, htons(ETH_P_ALL));      /*  较旧的系统*/
```

以上将返回该数据链路接收的所有协议的帧。 如果只想要IPv4的帧，调用将是

```c
fd = socket(PF_PACKET, SOCK_RAW, htons(ETH_P_IP));          /* 较新的系统 */
or

fd = socket(AF_INET, SOCK_PACKET, htons(ETH_P_IP));         /* 较旧的系统 */
```
最后一个参数的其他常量, 例如是 `ETH_P_ARP` 和 `ETH_P_IPV6`. 

指定一个 `ETH_P_xxx` 的协议告诉数据链路，对于数据链路收到的帧，要把哪些帧类型传递给套接字。如果数据链路支持混杂模式（例如，以太网），那么设备也必须进入混杂模式。这可以通过 `PACKET_ADD_MEMBERSHIP` 套接字选项完成，使用一个 `packet_mreq` 结构，指定一个接口和一个 `PACKET_MR_PROMISC` 的动作。在旧系统中，这是通过 `SIOCGIFFLAGS` 的 `ioctl` 来完成的，以获取标志，设置 `IFF_PROMISC` 标志，然后使用 `SIOCSIFFLAGS` 存储标志。不幸的是，使用这种方法，多个混杂监听器可能会相互干扰，而且一个有缺陷, 在退出后还会留下混杂模式


# TCP 协议

## 服务端准备连接的过程
### 创建套接字
```c
int socket(int domain, int type, int protocol)
```
- domain 就是指 PF_INET、PF_INET6 以及 PF_LOCAL 等，表示什么样的套接字。
- type 可用的值是：
  - **SOCK_STREAM**: 表示的是字节流，对应 **TCP**；
  - **SOCK_DGRAM**： 表示的是数据报，对应 **UDP**；
  - **SOCK_RAW**: 表示的是原始套接字。
- protocol 原本是用来指定通信协议的，但现在基本废弃。因为协议已经通过前面两个参数指定完成。protocol 目前一般写成 0 即可


### bind

  将socket 绑定到具体得ip:port 上

```c
bind(int fd, sockaddr * addr, socklen_t len)
```
第二个参数是通用地址格式sockaddr * addr。这里有一个地方值得注意，那就是虽然接收的是通用地址格式，实际上传入的参数可能是 IPv4、IPv6 或者本地套接字格式。bind 函数会根据 len 字段判断传入的参数 addr 该怎么解析，len 字段表示的就是传入的地址长度，它是一个可变值。

等同于
```c
bind(int fd, void * addr, socklen_t len)
```
不过当时没有 `void *`的支持, 为了解决这个问题设计了通用地址格式来作为支持 bind 和 accept 等这些函数的参数

```c
struct sockaddr_in name;
bind (sock, (struct sockaddr *) &name, sizeof (name)
```

根据该地址结构的前两个字节判断出是哪种地址。为了处理长度可变的结构，需要读取函数里的第三个参数，也就是 len 字段，这样就可以对地址进行解析和判断

#### 通用地址

起因：
  把地址设置成本机的 IP 地址，这相当告诉操作系统内核，仅仅对目标 IP 是本机IP 地址的 IP 包进行处理。但是这样写的程序在部署时有一个问题，编写应用程序时并不清楚自己的应用程序将会被部署到哪台机器上。可以利用通配地址的能力解决这个问题

  通配地址相当于告诉操作系统内核：“Hi，我可不挑活，只要目标地址是咱们的都可以。"比如一台机器有两块网卡，IP 地址分别是 202.61.22.55 和192.168.1.11，那么向这两个 IP 请求的请求包都会被我们编写的应用程序处理。

##### 设置通配地址
- 对于 IPv4 的地址来说，使用 INADDR_ANY 来完成通配地址的设置；
- 对于 IPv6 的地址来说，使用 IN6ADDR_ANY 来完成通配地址的设置。

```c
struct sockaddr_in name;
name.sin_addr.s_addr = htonl (INADDR_ANY); /* IPV4 通配地址 */
```

#### 通信端口
除了地址，还有端口。如果把端口设置成 0，就相当于把端口的选择权交给操作系统内核来处理，操作系统内核会根据一定的算法选择一个空闲的端口，完成套接字的绑定。这在服务器端不常使用。

```c
#include <stdio.h>
#include <stdlib.h>
#include <sys/socket.h>
#include <netinet/in.h>
int make_socket (uint16_t port)
{
  int sock;
  struct sockaddr_in name;
  /* 创建字节流类型的 IPV4 socket. */
  sock = socket (PF_INET, SOCK_STREAM, 0);
  if (sock < 0)
  {
    perror ("socket");
    exit (EXIT_FAILURE);
  }
  /* 绑定到 port 和 ip. */
  name.sin_family = AF_INET; /* IPV4 */
  name.sin_port = htons (port); /* 指定端口 */
  name.sin_addr.s_addr = htonl (INADDR_ANY); /* 通配地址 */
  /* 把 IPV4 地址转换成通用地址格式，同时传递长度 */
  if (bind (sock, (struct sockaddr *) &name, sizeof (name)) < 0)
  {
    perror ("bind");
    exit (EXIT_FAILURE);
  }
  
  return sock
} 
```

### listen  

  bind 函数只是让我们的套接字和地址关联，让服务器真正处于可接听的状态，这个过程需要依赖 listen 函数。 

初始化创建的套接字，可以认为是一个"主动"套接字，其目的是之后主动发起请求（通过调用 connect 函数，后面会讲到）。通过 listen 函数，可以将原来的"主动"套接字转换为"被动"套接字，告诉操作系统内核：“我这个套接字是用来等待用户请求的。"当然，操作系统内核会为此做好接收用户请求的一切准备，比如完成连接队列。

```c
int listen (int socketfd, int backlog)
```
- 第一个参数 socketfd 为套接字描述符
- 第二个参数 backlog 为未完成连接队列的大小，这个参数的大小决定了可以接收的并发数目。这个参数越大，并发数目理论上也会越大。但是参数过大也会占用过多的系统资源


### accept

  当客户端的连接请求到达时，服务器端应答成功，连接建立，这个时候操作系统内核需要把这个事件通知到应用程序，并让应用程序感知到这个连接。accept 这个函数的作用就是连接建立之后，操作系统内核和应用程序之间的桥梁。

```c
int accept(int listensockfd, struct sockaddr *cliaddr, socklen_t *addrlen)
```
- listensockfd 是套接字，可以叫它为 listen 套接字，因为这就是前面通过 bind，listen 一系列操作而得到的套接字
- cliadd 是通过指针方式获取的客户端的地址
- addrlen 地址的大小
- 函数的返回值是一个全新的描述字，代表了与客户端的连接

> 注意有两个套接字描述字，第一个是监听套接字描述字 listensockfd，它是作为输入参数存在的；第二个是返回的已连接套接字描述字。

#### 为什么要把两个套接字分开呢？
网络程序的一个重要特征就是并发处理，不可能一个应用程序运行之后只能服务一个客户; 所以监听套接字一直都存在，它是要为成千上万的客户来服务的，直到这个监听套接字关闭；而一旦一个客户和服务器连接成功，完成了 TCP 三次握手，操作系统内核就为这个客户生成一个已连接套接字，让应用服务器使用这个已连接套接字和客户进行通信处理。如果应用服务器完成了对这个客户的服务，那么关闭的就是
已连接套接字，这样就完成了 TCP 连接的释放。请注意，这个时候释放的只是这一个客户连接，其它被服务的客户连接可能还存在。最重要的是，监听套接字一直都处于“监听"状态，等待新的客户请求到达并服务。

## 客户端发起连接的过程
### 建立一个套接字

方法和前面服务端是一样的


### connect
  客户端和服务器端的连接建立

```c
int connect(int sockfd, const struct sockaddr *servaddr, socklen_t addrlen)
```
- 第一个参数 sockfd 是连接套接字，通过 socket 函数创建
- 第二个 servaddr 代表指向套接字地址结构的指针 
- 第三个参数 addrlen 代表指向套接字地址结构的大小

套接字地址结构必须含有服务器的 IP 地址和端口号。客户在调用函数 connect 前不必非得调用 bind 函数，因为如果需要的话，内核会确定源 IP 地址，并按照一定的算法选择一个临时端口作为源端口。

如果是 TCP 套接字，那么调用 `connect` 函数将激发 TCP 的**三次握手**过程，而且仅在连接建立成功或出错时才返回。其中出错返回可能有以下几种情况：
1. 三次握手无法建立，客户端发出的 `SYN` 包没有任何响应，于是返回 `TIMEOUT` 错误。这种情况比较常见的原因是对应的服务端 `IP` 写错。
2. 客户端收到了 `RST`（复位）回答，这时候客户端会立即返回 `CONNECTION REFUSED` 错误。这种情况比较常见于客户端发送连接请求时的请求端口写错，因为 `RST` 是 `TCP` 在发生错误时发送的一种 TCP 分节。产生 `RST` 的三个条件是：目的地为某端口的 `SYN` 到达，然而该端口上没有正在监听的服务器（如前所述）；TCP 想取消一个已有连接；TCP 接收到一个根本不存在的连接上的分节。
3. 客户发出的 `SYN` 包在网络上引起了 `"destination unreachable"`，即目的不可达的错误。这种情况比较常见的原因是客户端和服务器端路由不通。

#### TCP 三次握手

<img src="tcp 三次握手.png" style="zoom:80%;" >

最初的过程，服务器端通过 socket，bind 和 listen 完成了被动套接字的准备工作，被动的意思就是等着别人来连接，然后调用 accept，就会阻塞在这里，等待客户端的连接来临；客户端通过调用 socket 和 connect 函数之后，也会阻塞。接下来的事情是由操作系统内核完成的，更具体一点的说，是操作系统内核网络协议栈在工作。

具体的过程：
1. 客户端的协议栈向服务器端发送了 `SYN` 包，并告诉服务器端当前发送**序列号** `j`，客户端进入 `SYNC_SENT` 状态；
2. 服务器端的协议栈收到这个包之后，和客户端进行 `ACK` 应答，应答的值为 `j+1`，表示对 `SYN` 包 `j` 的确认，同时服务器也发送一个 `SYN` 包，告诉客户端当前我的发送序列号为 `k`，服务器端进入 `SYNC_RCVD` 状态；
3. 客户端协议栈收到 `ACK` 之后，使得应用程序从 `connect` 调用返回，表示客户端到服务器端的单向连接建立成功，客户端的状态为 `ESTABLISHED`，同时客户端协议栈也会对服务器端的 `SYN` 包进行应答，应答数据为 `k+1`；
4. 应答包到达服务器端后，服务器端协议栈使得 `accept` 阻塞调用返回，这个时候服务器端到客户端的单向连接也建立成功，服务器端也进入 `ESTABLISHED` 状态。 


### 发送数据
发送数据时常用的有三个函数，分别是 write、send 和 sendmsg。
```c
ssize_t write (int socketfd, const void *buffer, size_t size)
ssize_t send (int socketfd, const void *buffer, size_t size, int flags)
ssize_t sendmsg(int sockfd, const struct msghdr *msg, int flags)
```

使用场景：
- 第一个函数是常见的**文件写函数**，如果把 socketfd 换成文件描述符，就是普通的文件写入
- 如果**想指定选项，发送带外数据**，就需要使用第二个带 flag 的函数。所谓带外数据，是一种基于 TCP 协议的紧急数据，用于客户端 - 服务器在特定场景下的紧急处理。
- 如果**想指定多重缓冲区传输数据**，就需要使用第三个函数，以结构体 msghdr 的方式发送数据.

##### 发送缓冲区
当 TCP 三次握手成功，TCP 连接成功建立后，操作系统内核会为每一个连接创建配套的基础设施，比如发送缓冲区。发送缓冲区的大小可以通过套接字选项来改变，当应用程序调用 `write` 函数时，实际所做的事情是**把数据从应用程序中拷贝到操作系统内核的发送缓冲区中**，并不一定是把数据通过套接字写出去。

有几种情况：
- 第一种, 操作系统内核的发送缓冲区足够大，可以直接容纳这份数据，程序从 `write` 调用中退出，返回写入的字节数就是应用程序的数据大小。
- 第二种情况是，操作系统内核的发送缓冲区是够大了，不过还有数据没有发送完，或者数据发送完了，但是操作系统内核的发送缓冲区不足以容纳应用程序数据，在这种情况下，操作系统内核并不会返回，也不会报错，而是应用程序被阻塞，也就是说应用程序在 `write` 函数调用处停留，不直接返回。术语 **"挂起"** 也表达了相同的意思，不过 **"挂起"** 是从操作系统内核角度来说的。

##### 挂起什么时候才会返回呢？
大部分 UNIX 系统的做法是一直等到可以把应用程序数据完全放到操作系统内核的发送缓冲区中，再从系统调用中返回。

当 TCP 连接建立之后，它就开始运作起来。你可以把发送缓冲区想象成一条包裹流水线，有个聪明且忙碌的工人不断地从流水线上取出包裹（数据），这个工人会按照 `TCP/IP` 的语义，将取出的包裹（数据）封装成 TCP 的 `MSS` 包，以及 IP 的 `MTU`  包，最后走数据链路层将数据发送出去。这样发送缓冲区就又空了一部分，于是又可以继续从应用程序搬一部分数据到发送缓冲区里，这样一直进行下去，**到某一个时刻，应用程序的数据可以完全放置到发送缓冲区里。在这个时候，`write` 阻塞调用返回**。注意**返回的时刻，应用程序数据并没有全部被发送出去，发送缓冲区里还有部分数据，这部分数据会在稍后由操作系统内核通过网络发送出去**。

<img src="发送数据.png" style="zoom:80%;" >



### 读取数据

##### read 函数
```c
ssize_t read (int socketfd, void *buffer, size_t size)
```
`read` 函数要求操作系统内核从套接字描述字 `socketfd` 读取最多多少个字节（`size`），并将结果存储到 `buffer` 中。返回值告诉我们实际读取的字节数目，也有一些特殊情况，如果返回值为 `0`，表示 `EOF`（end-of-file），这在网络中表示对端发送了 `FIN` 包，要处理断连的情况；如果返回值为 `-1`，表示出错。当然，如果是非阻塞 I/O，情况会略有不同.

```c
/* 从 socketfd 描述字中读取 "size" 个字节. */
ssize_t readn(int fd, void *vptr, size_t size)
{
  size_t nleft;
  ssize_t nread;
  char *ptr;
  ptr = vptr;
  nleft = size;

  // 在没读满 size 个字节之前，一直都要循环下去
  while (nleft > 0) {
    if ( (nread = read(fd, ptr, nleft)) < 0) {
      // 非阻塞 I/O 的情况下，没有数据可以读，需要继续调用 read
      if (errno == EINTR)
        nread = 0; /* 这里需要再次调用 read */
      else
        return(-1);
    } else if (nread == 0) // 读到对方发出的 FIN 包，表现形式是 EOF，此时需要关闭套接字
      break; /* EOF(End of File) 表示套接字关闭 */

    // 读取的字符数减少，缓存指针往下移动
    nleft -= nread;
    ptr += nread;
  }
  // 在读取 EOF 跳出循环后，返回实际读取的字符数
  return(n - nleft); /* 返回的是实际读取的字节数 */
}
```

server.c
```c
int main(int argc, char **argv)
{
  int listenfd, connfd;
  socklen_t clilen;
  struct sockaddr_in cliaddr, servaddr;
  listenfd = socket(AF_INET, SOCK_STREAM, 0);
  bzero(&servaddr, sizeof(servaddr));

  servaddr.sin_family = AF_INET;
  servaddr.sin_addr.s_addr = htonl(INADDR_ANY);
  servaddr.sin_port = htons(12345);
  
  /* bind 到本地地址，端口为 12345 */
  bind(listenfd, (SA *) &servaddr, sizeof(servaddr));
  
  /* listen 的 backlog 为 1024 */
  listen(listenfd, 1024);
  
  /* 循环处理用户请求 */
  for ( ; ; ) {
    clilen = sizeof(cliaddr);
    connfd = accept(listenfd, (SA *) &cliaddr, &clilen);
    read_data(connfd); /* 读取数据 */
    close(connfd); /* 关闭连接套接字，注意不是监听套接字 */
  }
}


void read_data(int sockfd)
{
  ssize_t n;
  char buf[1024];
  int time = 0;

  for ( ; ; ) {
    fprintf(stdout, "block in read\n");
    if ( (n = Readn(sockfd, buf, 1024)) == 0)
      return; /* connection closed by other end */
    
    time ++;
    fprintf(stdout, "1K read for %d \n", time);
    usleep(1000);
  }
}
```


client.c
```c
int main(int argc, char **argv)
{
  int sockfd;
  struct sockaddr_in servaddr;
  if (argc != 2)
    err_quit("usage: tcpclient <IPaddress>");

  sockfd = socket(AF_INET, SOCK_STREAM, 0);
  bzero(&servaddr, sizeof(servaddr));

  servaddr.sin_family = AF_INET;
  servaddr.sin_port = htons(SERV_PORT);

  inet_pton(AF_INET, argv[1], &servaddr.sin_addr);

  connect(sockfd, (SA *) &servaddr, sizeof(servaddr));
  
  send_data(stdin, sockfd);
  
  exit(0);
}


# define MESSAGE_SIZE 10240000


void send_data(FILE *fp, int sockfd)
{
  char * query;
  query = malloc(MESSAGE_SIZE+1);

  for(int i=0; i< MESSAGE_SIZE; i++){
    query[i] = 'a';
  }
  query[MESSAGE_SIZE] = '\0';
  
  const char *cp;
  cp = query;
  remaining = strlen(query);
  
  while (remaining) {
    n_written = send(sockfd, cp, remaining, 0);
    fprintf(stdout, "send into buffer %ld \n", n_written);
    if (n_written <= 0) {
      perror("send");
      return;
    }

    remaining -= n_written;
    cp += n_written;
  }
  return;
}
```

测试：
- 客户端程序发送了一个很大的字节流， 客户端直到最后所有的字节流发送完毕才打印，说明在此之前 send 函数一直都是阻塞的，也就是说阻塞式套接字最终发送返回的实际写入字节数和请求字节数是相等的
- 服务端处理变慢, 把服务端的休眠时间稍微调大，把客户端发送的字节数从从 10240000 调整为 1024000 会发现，客户端很快打印. 但与此同时，服务端读取程序还在屏幕上不断打印读取数据的进度，显示出服务端读取程序还在辛苦地从缓冲区中读取数据。

发送成功仅仅表示的是数据被拷贝到了发送缓冲区中，并不意味着连接对端已经收到所有的数据。


> 对于 send 来说，返回成功仅仅表示数据写到发送缓冲区成功，并不表示对端已经成功收到。
> 对于 read 来说，需要循环读取数据，并且需要考虑 EOF 等异常条件。



# UDP (数据报协议)

<img src="UDP通信过程.png" style="zoom:70%;" >

服务器端创建 UDP 套接字之后，绑定到本地端口，调用 `recvfrom` 函数等待客户端的报文发送；客户端创建套接字之后，调用 `sendto` 函数往目标地址和端口发送 UDP 报文，然后客户端和服务器端进入互相应答过程.

## recvfrom 函数

```c
#include <sys/socket.h>

ssize_t recvfrom(int sockfd, void *buff, size_t nbytes, int flags, struct sockaddr *from, socklen_t *addrlen);
```
- `sockfd` 是本地创建的套接字描述符
- `buff` 指向本地的缓存
- `nbytes` 表示最大接收数据字节
- `flags` 是和 I/O 相关的参数，这里还用不到，设置为 `0`
- `from` 和 `addrlen`，实际上是返回**对端发送方的地址和端口等信息**, 这和TCP 非常不一样，TCP 是通过 accept 函数拿到的描述字信息来决定对端的信息。另外UDP 报文每次接收都会获取对端的信息，也就是说报文和报文之间是没有上下文的。
- 返回值代表实际接收的字节数




## sendto 函数
```c
#include <sys/socket.h>

ssize_t sendto(int sockfd, const void *buff, size_t nbytes, int flags, const struct sockaddr *to, socklen_t *addrlen);
```
- `sockfd` 是本地创建的套接字描述符
- `buff` 指向发送的缓存
- `nbytes` 表示发送字节数
- `flags` 依旧设置为 `0`
- `to` 和 `addrlen`，表示发送的**对端地址和端口等信息**
- 返回值实际发送的字节数

**server.c**

```c
#include "lib/common.h"
 
static int count;
 
static void recvfrom_int(int signo) {
    printf("\nreceived %d datagrams\n", count);
    exit(0);
}
 
 
int main(int argc, char **argv) {
    int socket_fd;
    socket_fd = socket(AF_INET, SOCK_DGRAM, 0); // 套接字类型是 "SOCK_DGRAM"，表示的是 UDP 数据报
 
    struct sockaddr_in server_addr;
    bzero(&server_addr, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    server_addr.sin_port = htons(SERV_PORT);
 
    // 绑定数据报套接字到本地的一个端口上
    bind(socket_fd, (struct sockaddr *) &server_addr, sizeof(server_addr));
 
    socklen_t client_len;
    char message[MAXLINE];
    count = 0;
 
    // 创建了一个信号处理函数，以便在响应 "Ctrl+C" 退出时，打印出收到的报文总数。
    signal(SIGINT, recvfrom_int);
 
    struct sockaddr_in client_addr;
    client_len = sizeof(client_addr);
    for (;;) {
        int n = recvfrom(socket_fd, message, MAXLINE, 0, (struct sockaddr *) &client_addr, &client_len);
        message[n] = 0;
        printf("received %d bytes: %s\n", n, message);
 
        char send_line[MAXLINE];
        sprintf(send_line, "Hi, %s", message);
 
        sendto(socket_fd, send_line, strlen(send_line), 0, (struct sockaddr *) &client_addr, client_len);
 
        count++;
    }
 
}
```

**client.c**

```c
#include "lib/common.h"
 
# define    MAXLINE     4096
 
int main(int argc, char **argv) {
    if (argc != 2) {
        error(1, 0, "usage: udpclient <IPaddress>");
    }
    
    int socket_fd;
    socket_fd = socket(AF_INET, SOCK_DGRAM, 0); // 创建一个类型为 "SOCK_DGRAM" 的套接字
 
    struct sockaddr_in server_addr;
    bzero(&server_addr, sizeof(server_addr));
    // 初始化目标服务器的地址和端口
    server_addr.sin_family = AF_INET;
    server_addr.sin_port = htons(SERV_PORT);
    inet_pton(AF_INET, argv[1], &server_addr.sin_addr);
 
    socklen_t server_len = sizeof(server_addr);
 
    struct sockaddr *reply_addr;
    reply_addr = malloc(server_len);
 
    char send_line[MAXLINE], recv_line[MAXLINE + 1];
    socklen_t len;
    int n;
 
    while (fgets(send_line, MAXLINE, stdin) != NULL) {
        int i = strlen(send_line);
        if (send_line[i - 1] == '\n') {
            send_line[i - 1] = 0;
        }
 
        printf("now sending %s\n", send_line);
        size_t rt = sendto(socket_fd, send_line, strlen(send_line), 0, (struct sockaddr *) &server_addr, server_len);
        if (rt < 0) {
            error(1, errno, "send failed ");
        }
        printf("send bytes: %zu \n", rt);
 
        len = 0;
        n = recvfrom(socket_fd, recv_line, MAXLINE, 0, reply_addr, &len);
        if (n < 0)
            error(1, errno, "recvfrom failed");
        recv_line[n] = 0;
        fputs(recv_line, stdout);
        fputs("\n", stdout);
    }
 
    exit(0);
}
```

测试:
- **只运行客户端**: `./udpclient 127.0.0.1` 发送阻塞, 对比TCP, 如果不开启服务端，TCP 客户端的 `connect` 函数会直接返回 `"Connection refused"` 报错信息。而在 UDP 程序里，则会一直阻塞在这里
- **先开启服务端，再开启客户端**: `./udpserver`, `./udpclient 127.0.0.1`; 客户端一次输入 g1、g2，服务器端在屏幕上打印出收到的字符，并且可以看到，客户端也收到了服务端的回应
- **开启服务端，再一次开启两个客户端**: 两个客户端发送的报文，依次都被服务端收到，并且客户端也可以收到服务端处理之后的报文; 服务器端重启后可以继续收到客户端的报文，这在 TCP 里是不可以的，TCP 断联之后必须重新连接才可以发送报文信息。但是 UDP 报文的 **"无连接"** 的特点，可以在 UDP 服务器重启之后，继续进行报文的发送，这就是 UDP 报文 **"无上下文"** 的最好说明。



# 本地套接字：

也叫做 UNIX 域套接字

**使用场景**：适用于在同一台主机上进程间通信的各种场景

-   本地套接字的编程接口和 IPv4、IPv6 套接字编程接口是一致的，可以支持字节流和数据报两种协议。
-   本地套接字的实现效率大大高于 IPv4 和 IPv6 的字节流、数据报套接字实现。

查看本地套接字列表：

```shell
# netstat -a -p -A unix
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   PID/Program name     Path
unix  2      [ ACC ]     STREAM     LISTENING     22272    1/systemd            /run/lvm/lvmetad.socket
unix  2      [ ACC ]     STREAM     LISTENING     71740    1080/hooagent        @hoo.conf.pub
unix  2      [ ACC ]     STREAM     LISTENING     22299    1/systemd            /run/lvm/lvmpolld.socket
unix  2      [ ACC ]     STREAM     LISTENING     61469    564/NetworkManager   /var/run/NetworkManager/private-dhcp
unix  2      [ ]         DGRAM                    54150953 7513/chronyd         /var/run/chrony/chronyd.sock
unix  3      [ ]         DGRAM                    7471     1/systemd            /run/systemd/notify
unix  2      [ ]         DGRAM                    7473     1/systemd            /run/systemd/cgroups-agent
unix  2      [ ACC ]     STREAM     LISTENING     54138855 644/master           private/rewrite
unix  2      [ ACC ]     STREAM     LISTENING     7491     1/systemd            /run/systemd/journal/stdout
unix  2      [ ]         DGRAM                    22341    1/systemd            /run/systemd/shutdownd
unix  6      [ ]         DGRAM                    7494     1/systemd            /run/systemd/journal/socket
unix  16     [ ]         DGRAM                    7496     1/systemd            /dev/log
unix  2      [ ACC ]     STREAM     LISTENING     72265    1/systemd            /var/run/docker.sock
unix  2      [ ACC ]     STREAM     LISTENING     230450255 1/systemd            /run/systemd/private
unix  2      [ ACC ]     STREAM     LISTENING     80469    1618/dockerd         /var/run/docker/metrics.sock
```

与TCP/UDP的区别：

​	TCP/UDP 即使在本地地址通信，也要走系统网络协议栈，而本地套接字，严格意义上说提供了一种单主机跨进程间调用的手段，减少了协议栈实现的复杂度，效率比 TCP/UDP 套接字都要高许多。类似的 IPC 机制还有 UNIX 管道、共享内存和 RPC 调用等。

## 本地字节流套接字：

**Server.c:**

```c
#include  "lib/common.h"
 
int main(int argc, char **argv) {
    if (argc != 2) {
        error(1, 0, "usage: unixstreamserver <local_path>");
    }
 
    int listenfd, connfd;
    socklen_t clilen;
    struct sockaddr_un cliaddr, servaddr;
 
    // TCP 的类型是 AF_INET 和字节流类型；UDP 的类型是 AF_INET 和数据报类型
    listenfd = socket(AF_LOCAL, SOCK_STREAM, 0);
    if (listenfd < 0) {
        error(1, errno, "socket create failed");
    }
 	// 设置一个本地文件路径, 必须是"绝对路径" 以便不同环境运行
    // 这个本地文件，必须是一个"文件"，不能是一个"目录"。如果文件不存在，后面 bind 操作时会自动创建这个文件。
    char *local_path = argv[1];
    // 把存在的文件删除掉, 保持幂等性
    unlink(local_path);
    bzero(&servaddr, sizeof(servaddr));
    servaddr.sun_family = AF_LOCAL;
    strcpy(servaddr.sun_path, local_path);
 
    if (bind(listenfd, (struct sockaddr *) &servaddr, sizeof(servaddr)) < 0) {
        error(1, errno, "bind failed");
    }
 
    if (listen(listenfd, LISTENQ) < 0) {
        error(1, errno, "listen failed");
    }
 
    clilen = sizeof(cliaddr);
    if ((connfd = accept(listenfd, (struct sockaddr *) &cliaddr, &clilen)) < 0) {
        if (errno == EINTR)
            error(1, errno, "accept failed");        /* back to for() */
        else
            error(1, errno, "accept failed");
    }
 
    char buf[BUFFER_SIZE];
 
    while (1) {
        bzero(buf, sizeof(buf));
        if (read(connfd, buf, BUFFER_SIZE) == 0) {
            printf("client quit");
            break;
        }
        printf("Receive: %s", buf);
 
        char send_line[MAXLINE];
        sprintf(send_line, "Hi, %s", buf);
 
        int nbytes = sizeof(send_line);
 
        if (write(connfd, send_line, nbytes) != nbytes)
            error(1, errno, "write error");
    }
 
    close(listenfd);
    close(connfd);
 
    exit(0);
}
```

**Client.c:**

```c
#include "lib/common.h"
 
int main(int argc, char **argv) {
    if (argc != 2) {
        error(1, 0, "usage: unixstreamclient <local_path>");
    }
 
    int sockfd;
    struct sockaddr_un servaddr;
 
    sockfd = socket(AF_LOCAL, SOCK_STREAM, 0);
    if (sockfd < 0) {
        error(1, errno, "create socket failed");
    }
 
    bzero(&servaddr, sizeof(servaddr));
    servaddr.sun_family = AF_LOCAL;
    strcpy(servaddr.sun_path, argv[1]);
 
    // 由于是本地套接字，并不会有三次握手
    if (connect(sockfd, (struct sockaddr *) &servaddr, sizeof(servaddr)) < 0) {
        error(1, errno, "connect failed");
    }
 
    char send_line[MAXLINE];
    bzero(send_line, MAXLINE);
    char recv_line[MAXLINE];
 
    while (fgets(send_line, MAXLINE, stdin) != NULL) {
 
        int nbytes = sizeof(send_line);
        if (write(sockfd, send_line, nbytes) != nbytes)
            error(1, errno, "write error");
 
        if (read(sockfd, recv_line, MAXLINE) == 0)
            error(1, errno, "server terminated prematurely");
 
        fputs(recv_line, stdout);
    }
 
    exit(0);
}
```

1.   **只启动客户端程序**: 由于没有启动服务器端，没有一个本地套接字在 /tmp/unixstream.sock 这个文件上监听，客户端直接报错，提示我们没有文件存在。

2.   **服务器端监听在无权限的文件路径上**: 提示`bind failed: Permission denied (13)`, 启动服务器端程序的用户，必须对本地监听路径有权限.

3.   **权限正常后启动**：本地路径下创建了一个本地文件，大小为 0，而且文件的最后结尾有一个（=）号。其实这就是 bind 的时候自动创建出来的文件。

     ```bash
     $ ls -al /var/lib/unixstream.sock
     rwxr-xr-x 1 root root 0 Jul 15 12:41 /var/lib/unixstream.sock=
     ```

     使用 netstat 命令查看 UNIX 域套接字，就会发现 unixstreamserver 这个进程，监听在 /var/lib/unixstream.sock 这个文件路径上。

4.   使用 Ctrol+C，让客户端程序退出时，服务器端也正常退出。

## 本地数据报套接字:

**Server.c:**

```c
#include  "lib/common.h"
 
int main(int argc, char **argv) {
    if (argc != 2) {
        error(1, 0, "usage: unixdataserver <local_path>");
    }
 
    int socket_fd;
    socket_fd = socket(AF_LOCAL, SOCK_DGRAM, 0);
    if (socket_fd < 0) {
        error(1, errno, "socket create failed");
    }
 
    struct sockaddr_un servaddr;
    char *local_path = argv[1];
    unlink(local_path);
    bzero(&servaddr, sizeof(servaddr));
    servaddr.sun_family = AF_LOCAL;
    strcpy(servaddr.sun_path, local_path);
 
    if (bind(socket_fd, (struct sockaddr *) &servaddr, sizeof(servaddr)) < 0) {
        error(1, errno, "bind failed");
    }
 
    char buf[BUFFER_SIZE];
    struct sockaddr_un client_addr;
    socklen_t client_len = sizeof(client_addr);
    while (1) {
        bzero(buf, sizeof(buf));
        if (recvfrom(socket_fd, buf, BUFFER_SIZE, 0, (struct sockadd *) &client_addr, &client_len) == 0) {
            printf("client quit");
            break;
        }
        printf("Receive: %s \n", buf);
 
        char send_line[MAXLINE];
        bzero(send_line, MAXLINE);
        sprintf(send_line, "Hi, %s", buf);
 
        size_t nbytes = strlen(send_line);
        printf("now sending: %s \n", send_line);
 
        if (sendto(socket_fd, send_line, nbytes, 0, (struct sockadd *) &client_addr, client_len) != nbytes)
            error(1, errno, "sendto error");
    }
 
    close(socket_fd);
 
    exit(0);
}
```

**Client.c:**

```c
#include "lib/common.h"
 
int main(int argc, char **argv) {
    if (argc != 2) {
        error(1, 0, "usage: unixdataclient <local_path>");
    }
 
    int sockfd;
    struct sockaddr_un client_addr, server_addr;
 
    sockfd = socket(AF_LOCAL, SOCK_DGRAM, 0);
    if (sockfd < 0) {
        error(1, errno, "create socket failed");
    }
 
    bzero(&client_addr, sizeof(client_addr));        /* bind an address for us */
    client_addr.sun_family = AF_LOCAL;
    strcpy(client_addr.sun_path, tmpnam(NULL));
 
    if (bind(sockfd, (struct sockaddr *) &client_addr, sizeof(client_addr)) < 0) {
        error(1, errno, "bind failed");
    }
 
    bzero(&server_addr, sizeof(server_addr));
    server_addr.sun_family = AF_LOCAL;
    strcpy(server_addr.sun_path, argv[1]);
 
    char send_line[MAXLINE];
    bzero(send_line, MAXLINE);
    char recv_line[MAXLINE];
 
    while (fgets(send_line, MAXLINE, stdin) != NULL) {
        int i = strlen(send_line);
        if (send_line[i - 1] == '\n') {
            send_line[i - 1] = 0;
        }
        size_t nbytes = strlen(send_line);
        printf("now sending %s \n", send_line);
 
        if (sendto(sockfd, send_line, nbytes, 0, (struct sockaddr *) &server_addr, sizeof(server_addr)) != nbytes)
            error(1, errno, "sendto error");
 
        int n = recvfrom(sockfd, recv_line, MAXLINE, 0, NULL, NULL);
        recv_line[n] = 0;
 
        fputs(recv_line, stdout);
        fputs("\n", stdout);
    }
 
    exit(0);
}
```

### 和UDP客户端不同之处：

​	和 UDP 网络编程的例子有一个非常大的不同, 将本地套接字 bind 到本地一个路径上，然而 UDP 客户端程序是不需要这么做的。本地数据报套接字这么做的原因是，它需要指定一个本地路径，以便在服务器端回包时，可以正确地找到地址；而在 UDP 客户端程序里，数据是可以通过 UDP 包的本地地址和端口来匹配的。



利用`netstat` 查看套接字：

```bash
# netstat -alepn
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       User       Inode      PID/Program name
tcp        0      0 0.0.0.0:9000            0.0.0.0:*               LISTEN      52250      199996534  22038/clickhouse-se
tcp        0      0 0.0.0.0:9004            0.0.0.0:*               LISTEN      52250      199996536  22038/clickhouse-se
tcp        0      0 0.0.0.0:9009            0.0.0.0:*               LISTEN      52250      199996535  22038/clickhouse-se
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      0          54139614   507/sshd
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      0          54138837   644/master

# 意思是本地 127.0.0.1 的端口 52464 连上本地 127.0.0.1 的端口 2379，状态为 ESTABLISHED，本地进程为 etcd，进程为 3496
tcp        0      0 127.0.0.1:2379          127.0.0.1:52464         ESTABLISHED 0    		27710       3496/etcd


// 只对 UNIX 套接字进行筛查
# netstat Socket -x -alepn
# /var/run/docker.sock 是本地套接字监听地址，dockerd 是进程名称，1400 是进程号
unix  3      [ ]         STREAM     CONNECTED     23209    1400/dockerd        /var/run/docker.sock

```



使用`lsof`找出正在使用该端口的那个进: 

```bash
# lsof -i :8080
```



# TIME_WAIT:

<img src="Tcp四次挥手.png" alt="Tcp四次挥手" style="zoom:50%;" />

## 产生过程：

​	TCP 连接终止时（即四次挥手），主机 1 先发送 FIN 报文，主机 2 进入 CLOSE_WAIT 状态，并发送一个 ACK 应答，同时，主机 2 通过 read 调用获得 EOF，并将此结果通知应用程序进行主动关闭操作，发送 FIN 报文。主机 1 在接收到 FIN 报文后发送 ACK 应答，此时主机 1 进入 TIME_WAIT 状态。

## 停留时间：

​	主机 1 在 TIME_WAIT 停留持续时间是固定的，是最长分节生命期 MSL（maximum segment lifetime）的两倍，一般称之为 2MSL。Linux 系统里有一个硬编码的字段，名称为`TCP_TIMEWAIT_LEN`，其值为 60 秒。也就是说，**Linux 系统停留在 TIME_WAIT 的时间为固定的 60 秒。**过了这个时间之后，主机 1 就进入 CLOSED 状态.

```c
#define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT state, about 60 seconds	*/
```

>   **只有发起连接终止的一方会进入 TIME_WAIT 状态**。

## 作用:

1.   为了确保最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭

     如果主机 1 没有维护 TIME_WAIT 状态，而直接进入 CLOSED 状态，它就失去了当前状态的上下文，只能回复一个 RST 操作，从而导致被动关闭方出现错误。

     现在主机 1 知道自己处于 TIME_WAIT 的状态，就可以在接收到 FIN 报文之后，重新发出一个 ACK 报文，使得主机 2 可以进入正常的 CLOSED 状态。

2.   让旧连接的重复分节在网络中自然消失

​		在原连接中断后，又重新创建了一个原连接的“化身"，说是化身其实是因为这个连接和原先的连接四元组完全相同，如果迷失报文经过一段时间也到达，那么这个报文会被误认为是连接“化身"的一个 TCP 分节，这样就会对 TCP 通信产生影响。**经过 2MSL 这个时间，足以让两个方向上的分组都被丢弃，使得原来连接的分组在网络中都自然消失**，再出现的分组一定都是新化身所产生的。

>   2MSL 的时间是**从主机 1 接收到 FIN 后发送 ACK 开始计时的**；如果在 TIME_WAIT 时间内，因为主机 1 的 ACK 没有传输到主机 2，主机 1 又接收到了主机 2 重发的 FIN 报文，那么 2MSL 时间将重新计时。2MSL 的时间，目的是为了让旧连接的所有报文都能自然消亡，现在主机 1 重新发送了 ACK 报文，自然需要重新计时，以便防止这个 ACK 报文对新可能的连接化身造成干扰。

## 危害:

过多的 TIME_WAIT 的主要危害有两种。

1.   内存资源占用

2.   对端口资源的占用:  

     端口资源也是有限的，一般可以开启的端口为 32768～61000 ，也可以通过`net.ipv4.ip_local_port_range`指定，如果 TIME_WAIT 状态过多，会导致无法创建新连接。

## 优化:

1.   通过 sysctl 命令，将系统值`net.ipv4.tcp_max_tw_buckets`调小。这个值默认为 18000，当系统中处于 TIME_WAIT 的连接一旦超过这个值时，系统就会将所有的 TIME_WAIT 连接状态**重置**，并且只打印出警告信息。

2.   调低 `TCP_TIMEWAIT_LEN` 重新编译系统.

3.   通过设置套接字选项`SO_LINGER`，来设置调用 close 或者 shutdown 关闭连接时的行为。

     ```c
     int setsockopt(int sockfd, int level, int optname, const void *optval,
     　　　　　　　　socklen_t optlen);
     
     struct linger {
     　int　 l_onoff;		/* 0=off, nonzero=on */
     　int　 l_linger;		/* linger time, POSIX specifies units as seconds */
     }
     ```

     设置 linger 参数有几种可能：

     -   如果`l_onoff`为 0，那么关闭本选项。`l_linger`的值被忽略，这对应了默认行为，close 或 shutdown 立即返回。如果在套接字发送缓冲区中有数据残留，系统会将试着把这些数据发送出去。
     -   如果`l_onoff`为非 0， 且`l_linger`值也为 0，那么调用 close 后，**会立该发送一个 RST 标志给对端，该 TCP 连接将跳过四次挥手，也就跳过了 TIME_WAIT 状态，直接关闭**。这种关闭的方式称为 "强行关闭"。 在这种情况下，排队数据不会被发送，被动关闭方也不知道对端已经彻底断开。只有当被动关闭方正阻塞在`recv()`调用上时，接受到 RST 时，会立刻得到一个"connet reset by peer" 的异常。

     ```c
     struct linger so_linger;
     so_linger.l_onoff = 1;
     so_linger.l_linger = 0;
     setsockopt(s,SOL_SOCKET,SO_LINGER, &so_linger,sizeof(so_linger));
     ```

     -   如果`l_onoff`为非 0， 且`l_linger`的值也非 0，那么调用 close 后，调用 close 的线程就将阻塞，直到数据被发送出去，或者设置的`l_linger`计时时间到。

4.   复用处于 TIME_WAIT 的套接字为新的连接所用 (**更安全的设置**) -- `net.ipv4.tcp_timestamps=1` 选项

     -   只适用于连接发起方（C/S 模型中的客户端）；

     -   对应的 TIME_WAIT 状态的连接创建时间超过 1 秒才可以被复用。

     `net.ipv4.tcp_timestamps=1`（默认即为 1）

     >   RFC 1323 中实现了 TCP 拓展规范，以便保证 TCP 的高可用，并引入了新的 TCP 选项，两个 4 字节的时间戳字段，用于记录 TCP 发送方的当前时间戳和从对端接收到的最新时间戳。由于引入了时间戳，那么 2MSL 问题就不复存在了，因为重复的数据包会因为时间戳过期被自然丢弃。



# 优雅关闭：

## 引用计数:

​	每个套接字都设置了一个积分，如果我们通过 fork 的方式产生子进程，套接字就会积分 +1， 如果我们调用一次 close 函数，套接字积分就会 -1。这就是套接字引用计数的含义。

## close 函数:

```c
int close(int sockfd)
```

close 函数具体是如何关闭两个方向的数据流呢？

-   在输入方向，系统内核会将该套接字设置为不可读，任何读操作都会返回异常。

-   在输出方向，系统内核尝试将发送缓冲区的数据发送给对端，并最后向对端发送一个 FIN 报文，接下来如果再对该套接字进行写操作会返回异常。如果对端没有检测到套接字已关闭，还继续发送报文，就会收到一个 RST 报文，告诉对端：“Hi, 我已经关闭了，别再给我发数据了“。

**特征：**

-   close 函数只是把套接字引用计数减 1，未必会立即关闭连接；
-   close 函数如果在套接字引用计数达到 0 时，立即终止读和写两个方向的数据传送。

## shutdown 函数:

```c
int shutdown(int sockfd, int howto)
```

对已连接的套接字执行 shutdown 操作，若成功则为 0，若出错则为 -1。

`howto` 是这个函数的设置选项，它的设置有三个主要选项：

-   `SHUT_RD(0)`：关闭连接的**"读"**这个方向，对该套接字进行读操作直接返回 `EOF`。从数据角度来看，套接字上接收缓冲区已有的数据将被丢弃，如果再有新的数据流到达，会对数据进行 `ACK`，然后悄悄地丢弃。也就是说，**对端还是会接收到 ACK，在这种情况下根本不知道数据已经被丢弃了**。
-   `SHUT_WR(1)`：关闭连接的**"写"**这个方向，这就是常被称为**"半关闭"**的连接。此时，不管套接字引用计数的值是多少，都会直接关闭连接的写方向。套接字上发送缓冲区已有的数据将被立即发送出去，并发送一个 `FIN` 报文给对端。应用程序如果对该套接字进行写操作会报错。
-   `SHUT_RDWR(2)`：相当于` SHUT_RD` 和` SHUT_WR` 操作各一次，关闭套接字的读和写两个方向。



## close 和 shutdown 的差别:

1.   close 会关闭连接，并释放所有连接对应的资源，而 shutdown 并不会释放掉套接字和所有的资源。

2.   close 存在引用计数的概念，并不一定导致该套接字不可用；shutdown 则不管引用计数，直接使得该套接字不可用，如果有别的进程企图使用该套接字，将会受到影响。

3.   close 的引用计数导致不一定会发出 `FIN` 结束报文，而 shutdown 则总是会发出 FIN 结束报文，这在我们打算关闭连接通知对端的时候，是非常重要的。

**Client.c**

```c
# include "lib/common.h"
# define    MAXLINE     4096
 
int main(int argc, char **argv) {
    if (argc != 2) {
        error(1, 0, "usage: graceclient <IPaddress>");
    }
    
    int socket_fd;
    // ipv4 TCP
    socket_fd = socket(AF_INET, SOCK_STREAM, 0);
 
    struct sockaddr_in server_addr;
    bzero(&server_addr, sizeof(server_addr));
    server_addr.sin_family = AF_INET; // IPv4
    server_addr.sin_port = htons(SERV_PORT);
    inet_pton(AF_INET, argv[1], &server_addr.sin_addr);
 
    socklen_t server_len = sizeof(server_addr);
    int connect_rt = connect(socket_fd, (struct sockaddr *) &server_addr, server_len);
    if (connect_rt < 0) {
        error(1, errno, "connect failed ");
    }
 
    char send_line[MAXLINE], recv_line[MAXLINE + 1];
    int n;
 
    fd_set readmask;
    fd_set allreads;
 
    FD_ZERO(&allreads);
    FD_SET(0, &allreads);
    FD_SET(socket_fd, &allreads);
    for (;;) {
        readmask = allreads;
        int rc = select(socket_fd + 1, &readmask, NULL, NULL, NULL);
        if (rc <= 0)
            error(1, errno, "select failed");
        if (FD_ISSET(socket_fd, &readmask)) {
            n = read(socket_fd, recv_line, MAXLINE);
            if (n < 0) {
                error(1, errno, "read error");
            } else if (n == 0) {
                error(1, 0, "server terminated \n");
            }
            recv_line[n] = 0;
            fputs(recv_line, stdout);
            fputs("\n", stdout);
        }
        if (FD_ISSET(0, &readmask)) {
            if (fgets(send_line, MAXLINE, stdin) != NULL) {
                if (strncmp(send_line, "shutdown", 8) == 0) {
                    FD_CLR(0, &allreads);
                    if (shutdown(socket_fd, 1)) {
                        error(1, errno, "shutdown failed");
                    }
                } else if (strncmp(send_line, "close", 5) == 0) {
                    FD_CLR(0, &allreads);
                    if (close(socket_fd)) {
                        error(1, errno, "close failed");
                    }
                    sleep(6);
                    exit(0);
                } else {
                    int i = strlen(send_line);
                    if (send_line[i - 1] == '\n') {
                        send_line[i - 1] = 0;
                    }
 
                    printf("now sending %s\n", send_line);
                    size_t rt = write(socket_fd, send_line, strlen(send_line));
                    if (rt < 0) {
                        error(1, errno, "write failed ");
                    }
                    printf("send bytes: %zu \n", rt);
                }
            }
        }
    }
}
```

**server.c**

```c
#include "lib/common.h"
 
static int count;
 
static void sig_int(int signo) {
    printf("\nreceived %d datagrams\n", count);
    exit(0);
}
 
int main(int argc, char **argv) {
    int listenfd;
    listenfd = socket(AF_INET, SOCK_STREAM, 0);
 
    // 设置了本地服务器 IPv4 地址，绑定到了 ANY 地址和指定的端口
    struct sockaddr_in server_addr;
    bzero(&server_addr, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    server_addr.sin_port = htons(SERV_PORT);
 
    int rt1 = bind(listenfd, (struct sockaddr *) &server_addr, sizeof(server_addr));
    if (rt1 < 0) {
        error(1, errno, "bind failed ");
    }
 
    int rt2 = listen(listenfd, LISTENQ);
    if (rt2 < 0) {
        error(1, errno, "listen failed ");
    }
 
    signal(SIGINT, sig_int);
    signal(SIGPIPE, SIG_IGN);
 
    int connfd;
    struct sockaddr_in client_addr;
    socklen_t client_len = sizeof(client_addr);
 
    if ((connfd = accept(listenfd, (struct sockaddr *) &client_addr, &client_len)) < 0) {
        error(1, errno, "bind failed ");
    }
 
    char message[MAXLINE];
    count = 0;
 
    for (;;) {
        int n = read(connfd, message, MAXLINE);
        if (n < 0) {
            error(1, errno, "error read");
        } else if (n == 0) {
            error(1, 0, "client closed \n");
        }
        message[n] = 0;
        printf("received %d bytes: %s\n", n, message);
        count++;
 
        char send_line[MAXLINE];
        sprintf(send_line, "Hi, %s", message);
 
        sleep(5);
 
        int write_nc = send(connfd, send_line, strlen(send_line), 0);
        printf("send bytes: %zu \n", write_nc);
        if (write_nc < 0) {
            error(1, errno, "error write");
        }
    }
}
```

<img src="关闭函数对比.png" style="zoom:50%;" >

可以注册一个信号处理函数，对 SIGPIPE 信号进行处理，避免程序莫名退出：

```c
static void sig_pipe(int signo) {
    printf("\nreceived %d datagrams\n", count);
    exit(0);
}
signal(SIGINT, sig_pipe);
```

>   在期望关闭连接其中一个方向时，应该使用 shutdown 函数 



# 连接有效性的检测:

## TCP Keep-Alive 

**原理：**

​	定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作用，每隔一个时间间隔，发送一个探测报文，该探测报文包含的数据非常少，如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序。

-   保活时间：`net.ipv4.tcp_keepalive_time` 默认 7200 秒（2 小时）
-   保活时间间隔：`net.ipv4.tcp_keepalive_intvl` 默认75 秒
-   保活探测次数：`net.ipv4.tcp_keepalve_probes` 默认 9 次



TCP 保活机制默认是关闭的，当选择打开时，可以分别在连接的两个方向上开启，也可以单独在一个方向上开启。如果开启服务器端到客户端的检测，就可以在客户端非正常断连的情况下清除在服务器端保留的“脏数据"；而开启客户端到服务器端的检测，就可以在服务器无响应的情况下，重新发起连接。



如果使用 TCP 自身的 keep-Alive 机制，在 Linux 系统中，最少需要经过 2 小时 11 分 15 秒才可以发现一个“死亡"连接。这个时间是怎么计算出来的呢？其实是通过 2 小时，加上 75 秒乘以 9 的总和。实际上，对很多对时延要求敏感的系统中，这个时间间隔是不可接受的。



**使用定时器做探活：**

**client.c**

```c
typedef struct {
    u_int32_t type;
    char data[1024];
} messageObject;
 
#define MSG_PING          1
#define MSG_PONG          2
#define MSG_TYPE1        11
#define MSG_TYPE2        21
```



```c
#include "lib/common.h"
#include "message_objecte.h"
 
#define    MAXLINE     4096
#define    KEEP_ALIVE_TIME  10
#define    KEEP_ALIVE_INTERVAL  3
#define    KEEP_ALIVE_PROBETIMES  3
 
 
int main(int argc, char **argv) {
    if (argc != 2) {
        error(1, 0, "usage: tcpclient <IPaddress>");
    }
 	// 创建了 TCP 套接字
    int socket_fd;
    socket_fd = socket(AF_INET, SOCK_STREAM, 0);
	// 创建了 IPv4 目标地址，其实就是服务器端地址，注意这里使用的是传入参数作为服务器地址
    struct sockaddr_in server_addr;
    bzero(&server_addr, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_port = htons(SERV_PORT);
    inet_pton(AF_INET, argv[1], &server_addr.sin_addr);
 	// 向服务器端发起连接
    socklen_t server_len = sizeof(server_addr);
    int connect_rt = connect(socket_fd, (struct sockaddr *) &server_addr, server_len);
    if (connect_rt < 0) {
        error(1, errno, "connect failed ");
    }
 
    char recv_line[MAXLINE + 1];
    int n;
 
    fd_set readmask;
    fd_set allreads;
 
    struct timeval tv;
    int heartbeats = 0;
 	// 设置了超时时间为 KEEP_ALIVE_TIME，这相当于保活时间
    tv.tv_sec = KEEP_ALIVE_TIME;
    tv.tv_usec = 0;
 
    messageObject messageObject;
 	// 初始化 select 函数的套接字
    FD_ZERO(&allreads);
    FD_SET(socket_fd, &allreads);
    for (;;) {
        readmask = allreads;
        int rc = select(socket_fd + 1, &readmask, NULL, NULL, &tv); // 调用 select 函数，感知 I/O 事件
        if (rc < 0) {
            error(1, errno, "select failed");
        }
        if (rc == 0) {
            // 客户端已经在 KEEP_ALIVE_TIME 这段时间内没有收到任何对当前连接的反馈，于是发起 PING 消息
            if (++heartbeats > KEEP_ALIVE_PROBETIMES) {
                error(1, 0, "connection dead\n");
            }
            printf("sending heartbeat #%d\n", heartbeats);
            messageObject.type = htonl(MSG_PING);
            rc = send(socket_fd, (char *) &messageObject, sizeof(messageObject), 0);
            if (rc < 0) {
                error(1, errno, "send failure");
            }
            tv.tv_sec = KEEP_ALIVE_INTERVAL;
            continue;
        }
        // 在接收到服务器端程序之后的处理
        if (FD_ISSET(socket_fd, &readmask)) {
            n = read(socket_fd, recv_line, MAXLINE);
            if (n < 0) {
                error(1, errno, "read error");
            } else if (n == 0) {
                error(1, 0, "server terminated \n");
            }
            printf("received heartbeat, make heartbeats to 0 \n");
            heartbeats = 0;
            tv.tv_sec = KEEP_ALIVE_TIME;
        }
    }
}
```

**server.c**

```c
#include "lib/common.h"
#include "message_objecte.h"
 
static int count;
 
int main(int argc, char **argv) {
    if (argc != 2) {
        error(1, 0, "usage: tcpsever <sleepingtime>");
    }
 
    int sleepingTime = atoi(argv[1]);
 	// 创建一个本地 TCP 监听套接字
    int listenfd;
    listenfd = socket(AF_INET, SOCK_STREAM, 0);
 	// 绑定该套接字到本地端口和 ANY 地址上
    struct sockaddr_in server_addr;
    bzero(&server_addr, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    server_addr.sin_port = htons(SERV_PORT);
 
    int rt1 = bind(listenfd, (struct sockaddr *) &server_addr, sizeof(server_addr));
    if (rt1 < 0) {
        error(1, errno, "bind failed ");
    }
 	// 调用 listen 和 accept 完成被动套接字转换和监听
    int rt2 = listen(listenfd, LISTENQ);
    if (rt2 < 0) {
        error(1, errno, "listen failed ");
    }
 
    int connfd;
    struct sockaddr_in client_addr;
    socklen_t client_len = sizeof(client_addr);
 
    if ((connfd = accept(listenfd, (struct sockaddr *) &client_addr, &client_len)) < 0) {
        error(1, errno, "bind failed ");
    }
 
    messageObject message;
    count = 0;
 	// 从建立的连接套接字上读取数据，解析报文，根据消息类型进行不同的处理
    for (;;) {
        int n = read(connfd, (char *) &message, sizeof(messageObject));
        if (n < 0) {
            error(1, errno, "error read");
        } else if (n == 0) {
            error(1, 0, "client closed \n");
        }
 
        printf("received %d bytes\n", n);
        count++;
 
        switch (ntohl(message.type)) {
            case MSG_TYPE1 :
                printf("process  MSG_TYPE1 \n");
                break;
 
            case MSG_TYPE2 :
                printf("process  MSG_TYPE2 \n");
                break;
 			// 通过休眠来模拟响应是否及时，然后调用 send 函数发送一个 PONG 报文，向客户端表示"还活着“的意思
            case MSG_PING: {
                messageObject pong_message;
                pong_message.type = MSG_PONG;
                sleep(sleepingTime);
                ssize_t rc = send(connfd, (char *) &pong_message, sizeof(pong_message), 0);
                if (rc < 0)
                    error(1, errno, "send failure");
                break;
            }
 			// 异常处理，因为消息格式不认识，所以程序出错退出
            default :
                error(1, 0, "unknown message type (%d)\n", ntohl(message.type));
        }
    }
}
```



# 复用已关闭套接字：

**前提：** 当服务重启之后，总是碰到"Address in use"的报错信息，服务器程序不能很快地重启。

**原因：**

​	当连接的一方主动关闭连接，在接收到对端的 FIN 报文之后，主动关闭连接的一方会在 TIME_WAIT 这个状态里停留一段时间，这个时间大约为 2MSL。如果刚关闭服务后使用 netstat 去查看服务器程序所在主机的 TIME_WAIT 的状态连接，会发现有一个服务器程序生成的 TCP 连接，当前正处于 TIME_WAIT 状态。通过服务器端发起的关闭连接操作，引起了一个已有的 TCP 连接处于 TME_WAIT 状态，正是这个 TIME_WAIT 的连接，使得服务器重启时，继续绑定在 127.0.0.1 地址和 9527 端口上的操作，返回了**Address already in use**的错误。

**解决优化办法：**

**重用套接字选项：**一个 TCP 连接是通过四元组（源地址、源端口、目的地址、目的端口）来唯一确定的，如果每次 Telnet 客户端使用的本地端口都不同，就不会和已有的四元组冲突，也就不会有 TIME_WAIT 的新旧连接化身冲突的问题。

-   第一种优化是新连接 `SYN` 告知的初始序列号，一定比 `TIME_WAIT` 老连接的末序列号大，这样通过序列号就可以区别出新老连接。
-   第二种优化是开启了 `tcp_timestamps`，使得新连接的时间戳比老连接的时间戳大，这样通过时间戳也可以区别出新老连接。

这样一个 TIME_WAIT 的 TCP 连接可以忽略掉旧连接，重新被新的连接所使用。

```c
int on = 1;
setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &on, sizeof(on));
```

>   SO_REUSEADDR 套接字选项，允许启动绑定在一个端口，即使之前存在一个和该端口一样的连接；
>
>   SO_REUSEADDR 套接字选项还有一个作用，那就是本机服务器如果有多个地址，可以在不同地址上使用相同的端口提供服务。

在默认情况下，服务器端历经创建 socket、bind 和 listen 重启时，如果试图绑定到一个现有连接上的端口，bind 操作会失败，但是如果在创建 socket 和 bind 之间，使用上面的代码片段设置 SO_REUSEADDR 套接字选项，情况就会不同。

```c
int main(int argc, char **argv) {
    int listenfd;
    listenfd = socket(AF_INET, SOCK_STREAM, 0);
 
    struct sockaddr_in server_addr;
    bzero(&server_addr, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    server_addr.sin_port = htons(SERV_PORT);
 
    // 重用套接字
    int on = 1;
    setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &on, sizeof(on));
 
    int rt1 = bind(listenfd, (struct sockaddr *) &server_addr, sizeof(server_addr));
    if (rt1 < 0) {
        error(1, errno, "bind failed ");
    }
 
    int rt2 = listen(listenfd, LISTENQ);
    if (rt2 < 0) {
        error(1, errno, "listen failed ");
    }
 
    signal(SIGPIPE, SIG_IGN);
 
    int connfd;
    struct sockaddr_in client_addr;
    socklen_t client_len = sizeof(client_addr);
 
    if ((connfd = accept(listenfd, (struct sockaddr *) &client_addr, &client_len)) < 0) {
        error(1, errno, "bind failed ");
    }
 
    char message[MAXLINE];
    count = 0;
 
    for (;;) {
        int n = read(connfd, message, MAXLINE);
        if (n < 0) {
            error(1, errno, "error read");
        } else if (n == 0) {
            error(1, 0, "client closed \n");
        }
        message[n] = 0;
        printf("received %d bytes: %s\n", n, message);
        count++;
    }
}
```

TCP 连接是通过四元组唯一区分的，只要客户端不使用相同的源端口，连接服务器是没有问题的，即使使用了相同的端口，根据序列号或者时间戳，也是可以区分出新旧连接的。而且，TCP 的机制绝对不允许在相同的地址和端口上绑定不同的服务器，即使我们设置 SO_REUSEADDR 套接字选项，也不可能在 ANY 通配符地址下和端口 9527 上重复启动两个服务器实例。

**`tcp_tw_reuse` 和 `SO_REUSEADDR` 区别：**

-   `tcp_tw_reuse` 是内核选项，主要用在连接的发起方。TIME_WAIT 状态的连接创建时间超过 1 秒后，新的连接才可以被复用，注意，这里是连接的发起方. **缩短time_wait的时间**；
-   `SO_REUSEADDR` 是用户态的选项，SO_REUSEADDR 选项用来告诉操作系统内核，如果端口已被占用，但是 TCP 连接状态位于 TIME_WAIT ，可以重用端口。如果端口忙，而 TCP 处于其他状态，重用端口时依旧得到“Address already in use"的错误信息。注意，这里一般都是连接的服务方。**支持同一个port对应多个ip，解决的是bind时的问题.**


# 数据传输
关于接收端字节流，有两点需要注意：

- 第一，发送的顺序肯定是会保持的，也就是说，先调用 send 函数发送的字节，总在后调用 send 函数发送字节的前面，这个是由 TCP 严格保证的；

- 第二，如果发送过程中有 TCP 分组丢失，但是其后续分组陆续到达，那么 TCP 协议栈会缓存后续分组，直到前面丢失的分组到达，最终，形成可以被应用程序读取的数据流。

## 网络字节序
比如 0x0201，对应的二进制为 00000010000000001，那么两个字节的数据到底是先传 0x01，还是相反？

### 大端小端
- 将 0x02 高字节存放在起始地址，这个叫做大端字节序（Big-Endian）
- 将 0x01 低字节存放在起始地址，这个叫做小端字节序（Little-Endian）

<img src="大小端.png">

为了保证网络字节序一致，POSIX 标准提供了如下的转换函数：
```c
uint16_t htons (uint16_t hostshort)
uint16_t ntohs (uint16_t netshort)
uint32_t htonl (uint32_t hostlong)
uint32_t ntohl (uint32_t netlong)
```
注：
- n 代表的就是 network
- h 代表的是 host
- s 表示的是 short
- l 表示的是 long
分别表示 16 位和 32 位的整数

如果系统本身是大端字节序，和网络字节序一样，那么使用上述所有的函数进行转换的时候，结果都仅仅是一个空实现，直接返回。
```c
# if __BYTE_ORDER == __BIG_ENDIAN
/* The host byte order is the same as network byte order,
   so these functions are all just identity.  */
# define ntohl(x) (x)
# define ntohs(x) (x)
# define htonl(x) (x)
# define htons(x) (x)
```

## 报文格式

简易格式：
| :----: | :----: | :----: |
|消息长度|消息类型|请求正文|

首先 4 个字节大小的消息长度，其目的是将真正发送的字节流的大小显式通过报文告知接收端，接下来是 4 个字节大小的消息类型，而真正需要发送的数据则紧随其后。


发包：
```c
int main(int argc, char **argv) {
    if (argc != 2) {
        error(1, 0, "usage: tcpclient <IPaddress>");
    }
 
    int socket_fd;
    socket_fd = socket(AF_INET, SOCK_STREAM, 0);
 
    struct sockaddr_in server_addr;
    bzero(&server_addr, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_port = htons(SERV_PORT);
    inet_pton(AF_INET, argv[1], &server_addr.sin_addr);
 
    socklen_t server_len = sizeof(server_addr);
    int connect_rt = connect(socket_fd, (struct sockaddr *) &server_addr, server_len);
    if (connect_rt < 0) {
        error(1, errno, "connect failed ");
    }
 
    // 报文格式转化为结构体
    struct {
        u_int32_t message_length;
        u_int32_t message_type;
        char buf[128];
    } message;
 
    int n;
 
    // 从标准输入读入数据，分别对消息长度、类型进行了初始化，注意这里使用了 htonl 函数将字节大小转化为了网络字节顺序 
    while (fgets(message.buf, sizeof(message.buf), stdin) != NULL) {
        n = strlen(message.buf);
        message.message_length = htonl(n);
        message.message_type = 1;
        if (send(socket_fd, (char *) &message, sizeof(message.message_length) + sizeof(message.message_type) + n, 0) <
            0)
            error(1, errno, "send failure");
 
    }
    exit(0);
}
```

解包：
```c
static int count;
 
static void sig_int(int signo) {
    printf("\nreceived %d datagrams\n", count);
    exit(0);
}
 

// 读取报文预设大小的字节
size_t readn(int fd, void *buffer, size_t length) {
    size_t count;
    ssize_t nread;
    char *ptr;
 
    ptr = buffer;
    count = length;
    while (count > 0) {
        nread = read(fd, ptr, count);
 
        if (nread < 0) {
            if (errno == EINTR)
                continue;
            else
                return (-1);
        } else if (nread == 0)
            break;                /* EOF */
 
        count -= nread;
        ptr += nread;
    }
    return (length - count);        /* return >= 0 */
}


size_t read_message(int fd, char *buffer, size_t length) {
    u_int32_t msg_length;
    u_int32_t msg_type;
    int rc;
 
    // 调用 readn 函数获取 4 个字节的消息长度数据
    rc = readn(fd, (char *) &msg_length, sizeof(u_int32_t));
    if (rc != sizeof(u_int32_t))
        return rc < 0 ? -1 : 0;
    msg_length = ntohl(msg_length);
 
    // 调用 readn 函数获取 4 个字节的消息类型数据
    rc = readn(fd, (char *) &msg_type, sizeof(msg_type));
    if (rc != sizeof(u_int32_t))
        return rc < 0 ? -1 : 0;

    // 判断消息的长度是不是太大，如果大到本地缓冲区不能容纳，则直接返回错误
    if (msg_length > length) {
        return -1;
    }
 

    // 调用 readn 一次性读取已知长度的消息体
    rc = readn(fd, buffer, msg_length);
    if (rc != msg_length)
        return rc < 0 ? -1 : 0;
    return rc;
}


 
int main(int argc, char **argv) {
    int listenfd;
    listenfd = socket(AF_INET, SOCK_STREAM, 0);
 
    struct sockaddr_in server_addr;
    bzero(&server_addr, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    server_addr.sin_port = htons(SERV_PORT);
 
    int on = 1;
    setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &on, sizeof(on));
 
    int rt1 = bind(listenfd, (struct sockaddr *) &server_addr, sizeof(server_addr));
    if (rt1 < 0) {
        error(1, errno, "bind failed ");
    }
 
    int rt2 = listen(listenfd, LISTENQ);
    if (rt2 < 0) {
        error(1, errno, "listen failed ");
    }
 
    signal(SIGPIPE, SIG_IGN);
 
    int connfd;
    struct sockaddr_in client_addr;
    socklen_t client_len = sizeof(client_addr);
 
    if ((connfd = accept(listenfd, (struct sockaddr *) &client_addr, &client_len)) < 0) {
        error(1, errno, "bind failed ");
    }
 
    char buf[128];
    count = 0;
 
    while (1) {
        int n = read_message(connfd, buf, sizeof(buf));
        if (n < 0) {
            error(1, errno, "error read message");
        } else if (n == 0) {
            error(1, 0, "client closed \n");
        }
        buf[n] = 0;
        printf("received %d bytes: %s\n", n, buf);
        count++;
    }
 
    exit(0);
}
```




# netfilter 和 iptables

- netfilter 组件也称为内核空间（kernelspace），是内核的一部分，由一些信息包过滤表组成，这些表包含内核用来控制信息包过滤处理的规则集。
- iptables 组件是一种工具，也称为用户空间（userspace），它使插入、修改和除去信息包过滤表中的规则变得容易。

iptables包含4个表，5个链。其中表是按照对数据包的操作区分的，链是按照不同的Hook点来区分的，表和链实际上是netfilter的两个维度。


## Netfilter Hooks：
netfilter程序可以注册五个钩子。随着数据包通过堆栈，它们将触发已注册到这些钩子的内核模块。数据包将触发的钩子取决于数据包是传入还是传出、数据包的目的地以及数据包在前一点是被丢弃还是被拒绝。

- `NF_IP_PRE_ROUTING`：进入网络堆栈后，任何**传入**流量都会触发此挂钩。在做出任何关于**将数据包发送到何处的路由决策之前，会处理此挂钩**。
- `NF_IP_LOCAL_IN`：如果数据包的**目的地是本地系统**，则在路由传入数据包后触发此挂钩。
- `NF_IP_FORWARD`: 如果要将数据包**转发到另一台主机**，则在路由传入数据包后触发此挂钩。
- `NF_IP_LOCAL_OUT`：任何**本地创建的出站流量**一旦到达网络堆栈就会触发此挂钩。
- `NF_IP_POST_ROUTING`：在**路由发生之后和被放到线路上之前**，任何传出或转发的流量都会触发此钩子。

## 内置链的名字镜像它们所关联的netfilter钩子的名字
- `PREROUTING`: 数据包进入路由表之前，由 `NF_IP_PRE_ROUTING` 钩子触发.
- `INPUT`: 通过路由表后目的地为本机，由 `NF_IP_LOCAL_IN` 钩子触发.
- `FORWARD`: 通过路由表后，目的地不为本机，由 `NF_IP_FORWARD` 钩子触发.
- `OUTPUT`: 由本机产生，向外转发，由 `NF_IP_LOCAL_OUT` 钩子触发.
- `POSTROUTING`: 发送到网卡接口之前，由 `NF_IP_POST_ROUTING` 钩子触发.


## iptables提供的表：
- filter：一般的过滤功能，用于决定是让一个信息包继续到达它预定的目的地，还是拒绝它的请求。
- nat:用于nat功能（端口映射，地址映射等），用于实现网络地址转换规则。当信息包进入网络堆栈时，该表中的规则将决定是否以及如何修改信息包的源地址或目的地址，以便影响信息包和任何响应流量的路由方式。当不能直接访问时，这通常用于将数据包路由到网络。
- mangle:用于对特定数据包的修改，用于通过多种方式改变报文的IP头。例如，可以调整报文的TTL (Time to Live)值，延长或缩短报文能够维持的有效网络跳数。其他IP头可以用类似的方式改变。
这个表还可以在数据包上放置一个内部内核“标记”，以便在其他表和其他网络工具中进行进一步处理。这个标记不接触实际的包，而是将这个标记添加到内核对包的表示中。
- raw:有限级最高，设置raw时一般是为了不再让iptables做数据包的链接跟踪处理，提高性能，提供一种标记数据包的机制，以便选择退出连接跟踪。
- security：用于在数据包上设置内部 SELinux 安全上下文标记，这将影响 SELinux 或其他可以解释 SELinux 安全上下文的系统如何处理数据包。这些标记可以在每个数据包或每个连接的基础上应用。

> 表的处理优先级：raw > mangle > nat > filter; 默认表是filter

<img src="客户端上的链优先级.png">

<img src="路由器上的链优先级.png">

<img src="ip 五个链.png">

<img src="ip 链流程.png">

<img src="ip 表和链的关系.png">

![image](https://user-images.githubusercontent.com/32731294/189052738-6508e49a-a276-407d-b713-da0dbb2f610c.png)



通过用一种方法来注册一个任意回调函数，以便在给定阶段的每个传入数据包上执行。幸运的是，有一个名为 netfilter 的项目正好提供了这个功能！netfilter 的代码驻留在 Linux 内核中，并将所有这些扩展点（即钩子）添加到网络堆栈的不同阶段。值得注意的是，iptables只是用于配置 netfilter 钩子的几个用户空间前端工具之一。更要注意的是——netfilter 的功能不受网络（即IP）层的限制，例如，也可以修改以太网帧。然而，正如它的名字一样，ip表格专注于从网络（IP）及以上开始的层。

|Tables/Chains|PREROUTING|INPUT|FORWARD|OUTPUT|POSTROUTING|
| ---- | ---- | ---- | ---- | ---- | ---- |
|(routing decision)||||√||
|row|√|||√||
|(connection tracking enabled)|√|||√||
|mangle|√|√|√|√|√|
|nat (DNAT)|√|||√||
|(routing decision)|√|||√||
|filter||√|√|√||
|security||√|√|√||
|nat (SNAT)||√|||√|


## 链遍历顺序
- 发送到本地系统的入方向数据包:PREROUTING -> INPUT
- 发送到另一个主机的入方向数据包:PREROUTING -> FORWARD -> POSTROUTING
- 本地生成的报文:OUTPUT -> POSTROUTING


## iptables 链
```bash
# 添加规则 "LOG every packet" 到INPUT链
$ iptables --append INPUT --jump LOG

# 添加规则 "DROP every packet" 到INPUT链
$ iptables --append INPUT --jump DROP
```
向 INPUT阶段添加了多个回调，意味着必须定义回调的执行顺序。实际上，当一个新的数据包到达时，首先执行第一个添加的回调（LOG the packet），然后执行第二个回调（DROP the packet）。因此，所有的回调都排成了一条链！但是链是由它所在的逻辑阶段命名的。

### iptables 规则、目标和策略
```bash
# 阻断源IP 46.36.222.157 的报文 
# -A is a shortcut for --append
# -j is a shortcut for --jump
$ iptables -A INPUT -s 46.36.222.157 -j DROP

# 阻止传出的SSH连接
$ iptables -A OUTPUT -p tcp --dport 22 -j DROP

# 允许所有传入HTTP(S)连接
$ iptables -A INPUT -p tcp -m multiport --dports 80,443 \
  -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
$ iptables -A OUTPUT -p tcp -m multiport --dports 80,443 \
  -m conntrack --ctstate ESTABLISHED -j ACCEPT
```

```bash
# check the default policies
$ sudo iptables --list-rules  # or -S
-P INPUT ACCEPT
-P FORWARD ACCEPT
-P OUTPUT ACCEPT

# change policy for chain FORWARD to target DROP
iptables --policy FORWARD DROP  # or -P
```



# IO复用：

## 阻塞 I/O :

### 写操作:

​	write 函数返回的字节数，和输入的参数总是一样的。



## 非阻塞 I/O:

### write 操作:

​	如果套接字对应的接收缓冲区没有数据可读，在非阻塞情况下 read 调用会立即返回，一般返回 `EWOULDBLOCK` 或 `EAGAIN` 出错信息。在这种情况下，出错信息是需要小心处理，比如后面再次调用 read 操作，而不是直接作为错误直接返回。

### read 操作:

​	如果套接字的发送缓冲区已达到了极限，不能容纳更多的字节，那么操作系统内核会**尽最大可能**从应用程序拷贝数据到发送缓冲区中，并立即从 write 等函数调用中返回。可想而知，在拷贝动作发生的瞬间，有可能一个字符也没拷贝，有可能所有请求字符都被拷贝完成，那么这个时候就需要返回一个数值，告诉应用程序到底有多少数据被成功拷贝到了发送缓冲区中，应用程序需要再次调用 write 函数，以输出未完成拷贝的字节。

​	write 等函数是可以同时作用到阻塞 I/O 和非阻塞 I/O 上的，为了复用一个函数，处理非阻塞和阻塞 I/O 多种情况，**设计出了写入返回值，并用这个返回值表示实际写入的数据大小**。



- 区别：

​		非阻塞 I/O 需要这样：`拷贝→返回→再拷贝→再返回`。 而阻塞 I/O 需要这样：`拷贝→直到所有数据拷贝至发送缓冲区完成→返回`。

-   使用方法：

​		使用循环的方式来写入数据，以不用区别阻塞和非阻塞 I/O。只不过在阻塞 I/O 的情况下，循环只执行一次就结束了。

```c
/* 向文件描述符 fd 写入 n 字节数 */
ssize_t writen(int fd, const void * data, size_t n)
{
    size_t      nleft;
    ssize_t     nwritten;
    const char  *ptr;
 
    ptr = data;
    nleft = n;
    // 如果还有数据没被拷贝完成，就一直循环
    while (nleft > 0) {
        if ( (nwritten = write(fd, ptr, nleft)) <= 0) {
           /* 这里 EINTR 是非阻塞 non-blocking 情况下，通知我们再次调用 write() */
            if (nwritten < 0 && errno == EINTR)
                nwritten = 0;      
            else
                return -1;         /* 出错退出 */
        }
 
        /* 指针增大，剩下字节数变小 */
        nleft -= nwritten;
        ptr   += nwritten;
    }
    return n;
}
```



#### 函数使用对比：

<img src="read、write使用对比.png" style="zoom: 67%;" />

**read 和 write 注意点：**

1.  read 总是在接收缓冲区有数据时就立即返回，不是等到应用程序给定的数据充满才返回。当接收缓冲区为空时，阻塞模式会等待，非阻塞模式立即返回 -1，并有 EWOULDBLOCK 或 EAGAIN 错误。
2.  和 read 不同，阻塞模式下，write 只有在发送缓冲区足以容纳应用程序的输出字节时才返回；而非阻塞模式下，则是能写入多少就写入多少，并返回实际写入的字节数。
3.  阻塞模式下的 write 有个特例, 就是对方主动关闭了套接字，这个时候 write 调用会立即返回，并通过返回值告诉应用程序实际写入的字节数，如果再次对这样的套接字进行 write 操作，就会返回失败。失败是通过返回值 -1 来通知到应用程序的。



### accept：

​	当 accept 和 I/O 多路复用 select、poll 等一起配合使用时，如果在监听套接字上触发事件，说明有连接建立完成，此时调用 accept 肯定可以返回已连接套接字。这样看来，似乎把监听套接字设置为非阻塞，没有任何好处。

​	为了说明这个问题，构建一个客户端程序，其中最关键的是，一旦连接建立，设置 `SO_LINGER` 套接字选项，把 `l_onoff` 标志设置为 1，把 `l_linger` 时间设置为 0。这样，**连接被关闭时，TCP 套接字上将会发送一个 `RST`**。

```c
struct linger ling;
ling.l_onoff = 1; 
ling.l_linger = 0;
setsockopt(socket_fd, SOL_SOCKET, SO_LINGER, &ling, sizeof(ling));
close(socket_fd);
```



```c
if (FD_ISSET(listen_fd, &readset)) {
    printf("listening socket readable\n");
    // 休眠导致在监听套接字上有可读事件发生时，并没有马上调用 accept
    sleep(5);
    struct sockaddr_storage ss;
    socklen_t slen = sizeof(ss);
    int fd = accept(listen_fd, (struct sockaddr *) &ss, &slen);
```

​	由于客户端发生了 RST 分节，该连接被接收端内核从自己的已完成队列中删除了，此时再调用 accept，由于没有已完成连接（假设没有其他已完成连接），accept 一直阻塞，更为严重的是，该线程再也没有机会对其他 I/O 事件进行分发，相当于该服务器无法对新连接和其他 I/O 进行服务.  如果将监听套接字设为非阻塞，上述的情形就不会再发生。只不过对于 accept 的返回值，需要正确地处理各种看似异常的错误，例如忽略 EWOULDBLOCK、EAGAIN 等。



### connect：

​	在非阻塞 TCP 套接字上调用 connect 函数，会立即返回一个 `EINPROGRESS` 错误。TCP 三次握手会正常进行，应用程序可以继续做其他初始化的事情。当该连接建立成功或者失败时，通过 I/O 多路复用 select、poll 等可以进行连接的状态检测。

```c
#define MAX_LINE 1024
#define FD_INIT_SIZE 128
 
char rot13_char(char c) {
    if ((c >= 'a' && c <= 'm') || (c >= 'A' && c <= 'M'))
        return c + 13;
    else if ((c >= 'n' && c <= 'z') || (c >= 'N' && c <= 'Z'))
        return c - 13;
    else
        return c;
}
 
// 数据缓冲区
struct Buffer {
    int connect_fd;  		// 连接字
    char buffer[MAX_LINE];  // 实际缓冲
    size_t writeIndex;      // 缓冲写入位置
    size_t readIndex;       // 缓冲读取位置
    int readable;           // 是否可以读
};
 
struct Buffer *alloc_Buffer() {
    struct Buffer *buffer = malloc(sizeof(struct Buffer));
    if (!buffer)
        return NULL;
    buffer->connect_fd = 0;
    buffer->writeIndex = buffer->readIndex = buffer->readable = 0;
    return buffer;
}
 
void free_Buffer(struct Buffer *buffer) {
    free(buffer);
}
 
int onSocketRead(int fd, struct Buffer *buffer) {
    char buf[1024];
    int i;
    ssize_t result;
    while (1) {
        result = recv(fd, buf, sizeof(buf), 0);
        if (result <= 0)
            break;
 
        for (i = 0; i < result; ++i) {
            if (buffer->writeIndex < sizeof(buffer->buffer))
                buffer->buffer[buffer->writeIndex++] = rot13_char(buf[i]);
            if (buf[i] == '\n') {
                buffer->readable = 1;  // 缓冲区可以读
            }
        }
    }
 
    if (result == 0) {
        return 1;
    } else if (result < 0) {
        if (errno == EAGAIN)
            return 0;
        return -1;
    }
 
    return 0;
}
 
int onSocketWrite(int fd, struct Buffer *buffer) {
    while (buffer->readIndex < buffer->writeIndex) {
        ssize_t result = send(fd, buffer->buffer + buffer->readIndex, buffer->writeIndex - buffer->readIndex, 0);
        if (result < 0) {
            if (errno == EAGAIN)
                return 0;
            return -1;
        }
 
        buffer->readIndex += result;
    }
 
    if (buffer->readIndex == buffer->writeIndex)
        buffer->readIndex = buffer->writeIndex = 0;
 
    buffer->readable = 0;
 
    return 0;
}
 
int tcp_nonblocking_server_listen(int port) {
    int listenfd;
    listenfd = socket(AF_INET, SOCK_STREAM, 0);

    make_nonblocking(listenfd);

    struct sockaddr_in server_addr;
    bzero(&server_addr, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    server_addr.sin_port = htons(port);

    int on = 1;
    setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &on, sizeof(on));

    int rt1 = bind(listenfd, (struct sockaddr *) &server_addr, sizeof(server_addr));
    if (rt1 < 0) {
        error(1, errno, "bind failed ");
    }

    int rt2 = listen(listenfd, LISTENQ);
    if (rt2 < 0) {
        error(1, errno, "listen failed ");
    }

    signal(SIGPIPE, SIG_IGN);

    return listenfd;
}


int main(int argc, char **argv) {
    int listen_fd;
    int i, maxfd;
 
    struct Buffer *buffer[FD_INIT_SIZE];
    for (i = 0; i < FD_INIT_SIZE; ++i) {
        buffer[i] = alloc_Buffer();
    }
 	// 调用 fcntl 将监听套接字设置为非阻塞
    listen_fd = tcp_nonblocking_server_listen(SERV_PORT);
 
    fd_set readset, writeset, exset;
    FD_ZERO(&readset);
    FD_ZERO(&writeset);
    FD_ZERO(&exset);
 
    while (1) {
        maxfd = listen_fd;
 
        FD_ZERO(&readset);
        FD_ZERO(&writeset);
        FD_ZERO(&exset);
 
        // listener 加入 readset
        FD_SET(listen_fd, &readset);
 
        for (i = 0; i < FD_INIT_SIZE; ++i) {
            if (buffer[i]->connect_fd > 0) {
                if (buffer[i]->connect_fd > maxfd)
                    maxfd = buffer[i]->connect_fd;
                FD_SET(buffer[i]->connect_fd, &readset);
                if (buffer[i]->readable) {
                    FD_SET(buffer[i]->connect_fd, &writeset);
                }
            }
        }
 		// 调用 select 进行 I/O 事件分发处理
        if (select(maxfd + 1, &readset, &writeset, &exset, NULL) < 0) {
            error(1, errno, "select error");
        }
 
        if (FD_ISSET(listen_fd, &readset)) {
            printf("listening socket readable\n");
            sleep(5);
            struct sockaddr_storage ss;
            socklen_t slen = sizeof(ss);
            int fd = accept(listen_fd, (struct sockaddr *) &ss, &slen);
            // 处理新的连接套接字，注意这里也把连接套接字设置为非阻塞的
            if (fd < 0) {
                error(1, errno, "accept failed");
            } else if (fd > FD_INIT_SIZE) {
                error(1, 0, "too many connections");
                close(fd);
            } else {
                make_nonblocking(fd);
                if (buffer[fd]->connect_fd == 0) {
                    buffer[fd]->connect_fd = fd;
                } else {
                    error(1, 0, "too many connections");
                }
            }
        }
 
        for (i = 0; i < maxfd + 1; ++i) {
            int r = 0;
            if (i == listen_fd)
                continue;
            
 			// 处理连接套接字上的 I/O 读写事件
            if (FD_ISSET(i, &readset)) {
                r = onSocketRead(i, buffer[i]);
            }
            if (r == 0 && FD_ISSET(i, &writeset)) {
                r = onSocketWrite(i, buffer[i]);
            }
            if (r) {
                buffer[i]->connect_fd = 0;
                close(i);
            }
        }
    }
}
```

>   示例代码参考：https://github.com/froghui/yolanda/blob/master/chap-22



## 异步IO: aio

```c
#include "lib/common.h"
#include <aio.h>
 
const int BUF_SIZE = 512;
 
int main() {
    int err;
    int result_size;
 
    // 创建一个临时文件
    char tmpname[256];
    snprintf(tmpname, sizeof(tmpname), "/tmp/aio_test_%d", getpid());
    unlink(tmpname);
    int fd = open(tmpname, O_CREAT | O_RDWR | O_EXCL, S_IRUSR | S_IWUSR);
    if (fd == -1) {
        error(1, errno, "open file failed ");
    }
 
    char buf[BUF_SIZE];
    struct aiocb aiocb;
 
    // 初始化 buf 缓冲，写入的数据应该为 0xfafa 这样的,
    memset(buf, 0xfa, BUF_SIZE);
    memset(&aiocb, 0, sizeof(struct aiocb));
    aiocb.aio_fildes = fd;
    aiocb.aio_buf = buf;
    aiocb.aio_nbytes = BUF_SIZE;
 
    // 开始写
    if (aio_write(&aiocb) == -1) {
        printf(" Error at aio_write(): %s\n", strerror(errno));
        close(fd);
        exit(1);
    }
 
    // 因为是异步的，需要判断什么时候写完
    while (aio_error(&aiocb) == EINPROGRESS) {
        printf("writing... \n");
    }
 
    // 判断写入的是否正确
    err = aio_error(&aiocb);
    result_size = aio_return(&aiocb);
    if (err != 0 || result_size != BUF_SIZE) {
        printf(" aio_write failed() : %s\n", strerror(err));
        close(fd);
        exit(1);
    }
 
    // 下面准备开始读数据
    char buffer[BUF_SIZE];
    struct aiocb cb;
    cb.aio_nbytes = BUF_SIZE;
    cb.aio_fildes = fd;
    cb.aio_offset = 0;
    cb.aio_buf = buffer;
 
    // 开始读数据
    if (aio_read(&cb) == -1) {
        printf(" air_read failed() : %s\n", strerror(err));
        close(fd);
    }
 
    // 因为是异步的，需要判断什么时候读完
    while (aio_error(&cb) == EINPROGRESS) {
        printf("Reading... \n");
    }
 
    // 判断读是否成功
    int numBytes = aio_return(&cb);
    if (numBytes != -1) {
        printf("Success.\n");
    } else {
        printf("Error.\n");
    }
 
    // 清理文件句柄
    close(fd);
    return 0;
}
```





## 四种IO 对比：

![image-20220824223054823](/Users/mr.sun/Library/Application Support/typora-user-images/image-20220824223054823.png)

-   第一种阻塞 I/O 就是你去了书店，告诉老板你想要某本书，然后你就一直在那里等着，直到书店老板翻箱倒柜找到你想要的书。

-   第二种非阻塞 I/O 类似于你去了书店，问老板有没有一本书，老板告诉你没有，你就离开了。一周以后，你又来这个书店，再问这个老板，老板一查，有了，于是你买了这本书。

-   第三种基于非阻塞的 I/O 多路复用，你来到书店告诉老板：“老板，到货给我打电话吧，我再来付钱取书。”

-   第四种异步 I/O 就是你连去书店取书的过程也想省了，你留下地址，付了书费，让老板到货时寄给你，你直接在家里拿到就可以看了



## select：

```c
int select(int maxfd, fd_set *readset, fd_set *writeset, fd_set *exceptset, const struct timeval *timeout);
```

返回：若有就绪描述符则为其数目，若超时则为 0，若出错则为 -1

-   读描述符集合 readset

-   写描述符集合 writeset 

-   异常描述符集合 exceptset

    这三个分别通知内核，在哪些描述符上检测数据可以读，可以写和有异常发生; 三个描述符集合中的每一个都可以设置成空，这样就表示不需要内核进行相关的检测。

-   timeval 结构体时间:

    ```c
    struct timeval {
      long   tv_sec; /* seconds */ 
      long   tv_usec; /* microseconds */
    };
    ```

    第一个可能是设置成空 (NULL)，表示如果没有 I/O 事件发生，则 select 一直等待下去

    第二个可能是设置一个非零的值，这个表示等待固定的一段时间后从 select 阻塞调用中返回

    第三个可能是将 tv_sec 和 tv_usec 都设置成 0，表示根本不等待，检测完毕立即返回。这种情况使用得比较少。



设置描述符集合:

```c
a[maxfd-1], ..., a[1], a[0]
    
void FD_ZERO(fd_set *fdset);　　　　　　
void FD_SET(int fd, fd_set *fdset);　　
void FD_CLR(int fd, fd_set *fdset);　　　
int  FD_ISSET(int fd, fd_set *fdset);
```

-   FD_ZERO 用来将这个向量的所有元素都设置成 0；
-   FD_SET 用来把对应套接字 fd 的元素，a[fd] 设置成 1；
-   FD_CLR 用来把对应套接字 fd 的元素，a[fd] 设置成 0；
-   FD_ISSET 对这个向量进行检测，判断出对应套接字的元素 a[fd] 是 0 还是 1。

其中 0 代表不需要处理，1 代表需要处理



比如现在的 select 待测试的描述符集合是{0,1,4}，那么 maxfd 就是 5，为啥是 5，而不是 4 呢?

```c
// 描述字集合{0,1,4}，对应的 maxfd 是 5，而不是 4
a[4],a[3],a[2],a[1],a[0]
```

>   -   描述符基数是当前最大描述符 +1；
>   -   每次 select 调用完成之后，记得要重置待测试集合。

### 测试样例：

```c
int main(int argc, char **argv) {
    if (argc != 2) {
        error(1, 0, "usage: select01 <IPaddress>");
    }
    int socket_fd = tcp_client(argv[1], SERV_PORT);
 
    char recv_line[MAXLINE], send_line[MAXLINE];
    int n;
 
    fd_set readmask;
    fd_set allreads;
    // 初始化了一个描述符集合，这个描述符读集合是空的
    FD_ZERO(&allreads);
    // 使用 FD_SET 将描述符 0，即标准输入
    FD_SET(0, &allreads);
    // 连接套接字描述符 3 设置为待检测
    FD_SET(socket_fd, &allreads);
 
    // 循环检测
    for (;;) {
        // 每次测试完之后，重新设置待测试的描述符集合
        readmask = allreads;
        // 使用 socket_fd+1 来表示待测试的描述符基数。切记需要 +1
        int rc = select(socket_fd + 1, &readmask, NULL, NULL, NULL);
 
        if (rc <= 0) {
            error(1, errno, "select failed");
        }
 
        // 连接描述字准备好可读
        if (FD_ISSET(socket_fd, &readmask)) {
            n = read(socket_fd, recv_line, MAXLINE);
            if (n < 0) {
                error(1, errno, "read error");
            } else if (n == 0) {
                error(1, 0, "server terminated \n");
            }
            recv_line[n] = 0;
            fputs(recv_line, stdout);
            fputs("\n", stdout);
        }
 
        // 使用 FD_ISSET 来判断哪个描述符准备好可读
        if (FD_ISSET(STDIN_FILENO, &readmask)) {
            if (fgets(send_line, MAXLINE, stdin) != NULL) {
                int i = strlen(send_line);
                if (send_line[i - 1] == '\n') {
                    send_line[i - 1] = 0;
                }
 
                printf("now sending %s\n", send_line);
                size_t rt = write(socket_fd, send_line, strlen(send_line));
                if (rt < 0) {
                    error(1, errno, "write failed ");
                }
                printf("send bytes: %zu \n", rt);
            }
        }
    }
}
```



## poll：

```c
int poll(struct pollfd *fds, unsigned long nfds, int timeout); 
```

返回值：若有就绪描述符则为其数目，若超时则为 0，若出错则为 -1（当有错误发生时，poll 函数的返回值为 -1；如果在指定的时间到达之前没有任何事件发生，则返回 0，否则就返回检测到的事件个数，也就是"returned events"中非 0 的描述符个数）

-   Polled 数组

    ```c
    struct pollfd {
        int    fd;       /* 描述符 fd */
        short  events;   /* 描述符上待检测的事件类型 events, events 可以表示多个不同的事件，具体的实现可以通过使用二进制掩码位操作来完成 */
        short  revents;  /* events returned */
     };
    
    // events 取值举例：
    /*
    #define    POLLIN    0x0001    /* any readable data available */
    #define    POLLPRI   0x0002    /* OOB/Urgent readable data */
    #define    POLLOUT   0x0004    /* file descriptor is writeable */
    */
    ```

    -   events 类型的事件可以分为两大类

        -   第一类是可读事件，有以下几种：

            ```c
            #define POLLIN     0x0001    /* any readable data available */
            #define POLLPRI    0x0002    /* OOB/Urgent readable data */
            #define POLLRDNORM 0x0040    /* non-OOB/URG data available */
            #define POLLRDBAND 0x0080    /* OOB/Urgent readable data */
            ```

        -   第二类是可写事件，有以下几种：

            ```c
            #define POLLOUT    0x0004    /* file descriptor is writeable */
            #define POLLWRNORM POLLOUT   /* no write type differentiation */
            #define POLLWRBAND 0x0100    /* OOB/Urgent data can be written */
            ```

            一般使用 POLLIN、POLLOUT

-   参数 nfds 描述的是数组 fds 的大小，简单说，就是向 poll 申请的事件检测的个数
-   参数 timeout，描述了 poll 的行为
    -   如果是一个 <0 的数，表示在有事件发生之前永远等待；
    -   如果是 0，表示不阻塞进程，立即返回；
    -   如果是一个 >0 的数，表示 poll 调用方等待指定的毫秒数后返回



**不想对某个 pollfd 结构进行事件检测，**可以把它对应的 pollfd 结构的 fd 成员设置成一个负值。这样，poll 函数将忽略这样的 events 事件，检测完成以后，所对应 "returned events" 的成员值也将设置为 0



### 与 select 的区别：

​	在 select 里面，文件描述符的个数已经随着 fd_set 的实现而固定，没有办法对此进行配置；而在 poll 函数里，可以控制 pollfd 结构的数组大小，这意味着可以突破原来 select 函数最大描述符的限制，在这种情况下，应用程序调用者需要分配 pollfd 数组并通知 poll 函数该数组的大小。

​	和 select 非常不同的地方在于，poll 每次检测之后的结果不会修改原来的传入值，而是将结果保留在 revents 字段中，这样就不需要每次检测完都得重置待检测的描述字和感兴趣的事件。可以把 revents 理解成"returned events"。



### 测试用例：

```c
#define INIT_SIZE 128
 
int main(int argc, char **argv) {
    int listen_fd, connected_fd;
    int ready_number;
    ssize_t n;
    char buf[MAXLINE];
    struct sockaddr_in client_addr;
 	// 需要创建一个监听套接字，并绑定在本地的地址和端口上
    listen_fd = tcp_server_listen(SERV_PORT);
 
    // 初始化 pollfd 数组，这个数组的第一个元素是 listen_fd，其余的用来记录将要连接的 connect_fd	
    struct pollfd event_set[INIT_SIZE];
    event_set[0].fd = listen_fd;
    event_set[0].events = POLLRDNORM;
 
    // 用 -1 表示这个数组位置还没有被占用(对应 pollfd 里的文件描述字 fd 为负数，poll 函数将会忽略这个 pollfd)
    int i;
    for (i = 1; i < INIT_SIZE; i++) {
        event_set[i].fd = -1;
    }
 
    for (;;) {
        // INIT_SIZE 是因为 poll 函数已经能保证可以自动忽略 fd 为 -1 的 pollfd，不需要每次都计算一下 event_size 里真正需要被检测的元素大小
        // timeout 设置为 -1，表示在 I/O 事件发生之前 poll 调用一直阻塞
        if ((ready_number = poll(event_set, INIT_SIZE, -1)) < 0) {
            error(1, errno, "poll failed ");
        }
 
        // 和对应的事件类型进行位与操作，因为 event 都是通过二进制位来进行记录的，位与操作是和对应的二进制位进行操作，一个文件描述字是可以对应到多个事件类型的
        if (event_set[0].revents & POLLRDNORM) {
            socklen_t client_len = sizeof(client_addr);
            connected_fd = accept(listen_fd, (struct sockaddr *) &client_addr, &client_len);
 
            // 找到一个可以记录该连接套接字的位置
            for (i = 1; i < INIT_SIZE; i++) {
                if (event_set[i].fd < 0) {
                    event_set[i].fd = connected_fd;
                    event_set[i].events = POLLRDNORM;
                    break;
                }
            }
 
            // 如果在数组里找不到这样一个位置，说明 event_set 已经被很多连接充满了
            if (i == INIT_SIZE) {
                error(1, errno, "can not hold so many clients");
            }
 
            if (--ready_number <= 0)
                continue;
        }
 
        for (i = 1; i < INIT_SIZE; i++) {
            int socket_fd;
            if ((socket_fd = event_set[i].fd) < 0)
                continue;
            if (event_set[i].revents & (POLLRDNORM | POLLERR)) {
                if ((n = read(socket_fd, buf, MAXLINE)) > 0) {
                    if (write(socket_fd, buf, n) < 0) {
                        error(1, errno, "write error");
                    }
                } else if (n == 0 || errno == ECONNRESET) {
                    close(socket_fd);
                    event_set[i].fd = -1;
                } else {
                    error(1, errno, "read error");
                }
 
                if (--ready_number <= 0)
                    break;
            }
        }
    }
}
```

### 测试办法：

​	启动这个服务器程序，然后通过 telnet 连接到这个服务器程序	





### **开启方法：**

lib/event_loop.c 文件的 event_loop_init_with_name 函数是关键，这里是通过宏 **EPOLL_ENABLE** 来决定是使用 epoll 还是 poll   

```c
struct event_loop *event_loop_init_with_name(char *thread_name) {
  ...
#ifdef EPOLL_ENABLE
    yolanda_msgx("set epoll as dispatcher, %s", eventLoop->thread_name);
    eventLoop->eventDispatcher = &epoll_dispatcher;
#else
    yolanda_msgx("set poll as dispatcher, %s", eventLoop->thread_name);
    eventLoop->eventDispatcher = &poll_dispatcher;
#endif
    eventLoop->event_dispatcher_data = eventLoop->eventDispatcher->init(eventLoop);
    ...
}
```



**CMakeLists.txt**

```bash
cmake_minimum_required(VERSION 3.1)
project(yolanda C CXX)

set(CMAKE_CXX_STANDARD 11)
set(CMAKE_C_STANDARD 99)

# Put the libaries and binaries that get built into directories at the
# top of the build tree rather than in hard-to-find leaf directories.
set(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${PROJECT_BINARY_DIR}/bin)
set(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${PROJECT_BINARY_DIR}/lib)
set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${PROJECT_BINARY_DIR}/lib)

include_directories(${PROJECT_SOURCE_DIR})

# check epoll and add config.h for the macro compilation
# 引入 CheckSymbolExists，如果系统里有 epoll_create 函数和 sys/epoll.h，就自动开启 EPOLL_ENABLE。如果没有，EPOLL_ENABLE 就不会开启，自动使用 poll 作为默认的事件分发机制
include(CheckSymbolExists)
check_symbol_exists(epoll_create "sys/epoll.h" EPOLL_EXISTS)
if (EPOLL_EXISTS)
    # Linux下设置为epoll
    set(EPOLL_ENABLE 1 CACHE INTERNAL "enable epoll")
else ()
	# Linux下也设置为poll
    set(EPOLL_ENABLE "" CACHE INTERNAL "not enable epoll")
endif ()

# 为了能让编译器使用到这个宏，需要让 CMake 往 config.h 文件里写入这个宏的最终值，configure_file 命令就是起这个作用的
configure_file(${CMAKE_CURRENT_SOURCE_DIR}/config.h.cmake
        ${CMAKE_CURRENT_BINARY_DIR}/include/config.h)

include_directories(${CMAKE_CURRENT_BINARY_DIR}/include)
```



**config.h.cmake**

```bash
/* Define to 1 if you have the `epoll_create1' function. */
#cmakedefine EPOLL_ENABLE 1
```







## epoll：

### 触发方式：

#### 边缘触发：edge-triggered (ET)

​	边缘触发的意思是只有**第一次**满足条件的时候才触发，之后就不会再传递同样的事件

#### 水平触发：level-triggered (LT)

​	条件触发（水平触发）的意思是只要满足事件的条件，比如有数据需要读，就**一直**不断地把这个事件传递给用户



### 调用方法：

#### **epoll_create:** 

​	创建了一个 epoll 实例， 需要调用 close() 方法释放 epoll 实例

```c
int epoll_create(int size);
int epoll_create1(int flags);

返回值: 若成功返回一个大于 0 的值，表示 epoll 实例；若返回 -1 表示出错
```

​	**size**：初始用来告知内核期望监控的文件描述字大小，然后内核使用这部分的信息来初始化内核数据结构，后升级为内核可以动态分配需要的内核数据结构， 所以将 size 设置成一个大于 0 的整数即可。

​	如果 epoll_create1() 的输入 size 大小为 0，则和 epoll_create() 一样，内核自动忽略。可以增加如 EPOLL_CLOEXEC 的额外选项

#### **epoll_ctl : ** 

​	往epoll 实例增加或删除监控的事件

```c
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
返回值: 若成功返回 0；若返回 -1 表示出错
```

**epfd**: 是刚刚调用 epoll_create 创建的 epoll 实例描述字，可以简单理解成是 epoll 句柄

**op**: 表示增加还是删除一个监控事件，它有三个选项可供选择：

​		EPOLL_CTL_ADD： 向 epoll 实例注册文件描述符对应的事件；
​		EPOLL_CTL_DEL：向 epoll 实例删除文件描述符对应的事件；
​		EPOLL_CTL_MOD： 修改文件描述符对应的事件。

**fd:** 注册的事件的文件描述符，比如一个监听套接字.

**event**: 注册的事件类型，并且可以在这个结构体里设置用户需要的数据，其中最为常见的是使用联合结构里的 fd 字段，表示事件所对应的文件描述符

```c
typedef union epoll_data {
     void        *ptr;
     int          fd;
     uint32_t     u32;
     uint64_t     u64;
 } epoll_data_t;
 
 struct epoll_event {
     uint32_t     events;      /* Epoll events */
     epoll_data_t data;        /* User data variable */
 };
```



**事件类型：** 

-   EPOLLIN：表示对应的文件描述字可以读；

-   EPOLLOUT：表示对应的文件描述字可以写；

-   EPOLLRDHUP：表示套接字的一端已经关闭，或者半关闭；

-   EPOLLHUP：表示对应的文件描述字被挂起；

-   EPOLLET：设置为 edge-triggered，默认为 level-triggered。

    

#### **epoll_wait:**  

​	挂起调用的进程，在等待内核 I/O 事件的分发

```c
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);

返回值: 成功返回的是一个大于 0 的数，表示事件的个数；返回 0 表示的是超时时间到；若出错返回 -1.
```

​	**epfd**: 是刚刚调用 epoll_create 创建的 epoll 实例描述字， epoll 句柄

​	**events:** 返回给用户空间需要处理的 I/O 事件，这是一个**数组**，数组的大小由 epoll_wait 的返回值决定，这个数组的每个元素都是一个需要待处理的 I/O 事件，其中 events 表示具体的事件类型，事件类型取值和 epoll_ctl 可设置的值一样，这个 epoll_event 结构体里的 data 值就是在 epoll_ctl 那里设置的 data，也就是用户空间和内核空间调用时需要的数据

​	**maxevents：**是一个大于 0 的整数，表示 epoll_wait 可以返回的最大事件值 

​	**timeout**：是 epoll_wait 阻塞调用的超时值，如果这个值设置为 -1，表示不超时；如果设置为 0 则立即返回，即使没有任何 I/O 事件发生	



>   https://man7.org/linux/man-pages/man7/epoll.7.html



#### 源码理解：

**eventpoll 结构：**

​	这个数据结构是在调用 `epoll_create` 之后内核侧创建的一个句柄，表示了一个 epoll **实例,**  调用 `epoll_ctl` 和 `epoll_wait` 等，都是对这个 **eventpoll** 数据进行操作，这部分数据会被保存在`epoll_create`创建的匿名文件 file 的 **private_data** 字段中

```c
/*
 * This structure is stored inside the "private_data" member of the file
 * structure and represents the main data structure for the eventpoll
 * interface.
 */
struct eventpoll {
    /* Protect the access to this structure */
    spinlock_t lock;
 
    /*
     * This mutex is used to ensure that files are not removed
     * while epoll is using them. This is held during the event
     * collection loop, the file cleanup path, the epoll file exit
     * code and the ctl operations.
     */
    struct mutex mtx;
 
    /* Wait queue used by sys_epoll_wait() */
    // 这个队列里存放的是执行 epoll_wait 从而等待的进程队列
    wait_queue_head_t wq;
 
    /* Wait queue used by file->poll() */
    // 这个队列里存放的是该 eventloop 作为 poll 对象的一个实例，加入到等待的队列
    // 这是因为 eventpoll 本身也是一个 file, 所以也会有 poll 操作
    wait_queue_head_t poll_wait;
 
    /* List of ready file descriptors */
    // 这里存放的是事件就绪的 fd 列表，链表的每个元素是下面的 epitem
    struct list_head rdllist;
 
    /* RB tree root used to store monitored fd structs */
    // 这是用来快速查找 fd 的红黑树
    struct rb_root_cached rbr;
 
    /*
     * This is a single linked list that chains all the "struct epitem" that
     * happened while transferring ready events to userspace w/out
     * holding ->lock.
     */
    struct epitem *ovflist;
 
    /* wakeup_source used when ep_scan_ready_list is running */
    struct wakeup_source *ws;
 
    /* The user that created the eventpoll descriptor */
    struct user_struct *user;
 
    // 这是 eventloop 对应的匿名文件，充分体现了 Linux 下一切皆文件的思想
    struct file *file;
 
    /* used to optimize loop detection check */
    int visited;
    struct list_head visited_list_link;
 
#ifdef CONFIG_NET_RX_BUSY_POLL
    /* used to track busy poll napi_id */
    unsigned int napi_id;
#endif
};
```


**epitem 结构：**

​	调用 `epoll_ctl` 增加一个 `fd` 时，内核就会创建出一个 `epitem` 实例，并且把这个实例作为红黑树的一个子节点，增加到 `eventpoll` 结构体中的**红黑树**中，对应的字段是 `rbr`。这之后，查找每一个 `fd` 上是否有事件发生都是通过红黑树上的 `epitem` 来操作

```c
/*
 * Each file descriptor added to the eventpoll interface will
 * have an entry of this type linked to the "rbr" RB tree.
 * Avoid increasing the size of this struct, there can be many thousands
 * of these on a server and we do not want this to take another cache line.
 */
struct epitem {
    union {
        /* RB tree node links this structure to the eventpoll RB tree */
        struct rb_node rbn;
        /* Used to free the struct epitem */
        struct rcu_head rcu;
    };
 
    /* List header used to link this structure to the eventpoll ready list */
    // 将这个 epitem 连接到 eventpoll 里面的 rdllist 的 list 指针
    struct list_head rdllink;
 
    /*
     * Works together "struct eventpoll"->ovflist in keeping the
     * single linked chain of items.
     */
    struct epitem *next;
 
    /* The file descriptor information this item refers to */
    //epoll 监听的 fd
    struct epoll_filefd ffd;
 
    /* Number of active wait queue attached to poll operations */
    // 一个文件可以被多个 epoll 实例所监听，这里就记录了当前文件被监听的次数
    int nwait;
 
    /* List containing poll wait queues */
    struct list_head pwqlist;
 
    /* The "container" of this item */
    // 当前 epollitem 所属的 eventpoll
    struct eventpoll *ep;
 
    /* List header used to link this item to the "struct file" items list */
    struct list_head fllink;
 
    /* wakeup_source used when EPOLLWAKEUP is set */
    struct wakeup_source __rcu *ws;
 
    /* The structure that describe the interested events and the source fd */
    struct epoll_event event;
};
```



**eppoll_entry 结构：**

​	每次当一个 fd 关联到一个 epoll 实例，就会有一个 eppoll_entry 产生

```c
/* Wait structure used by the poll hooks */
struct eppoll_entry {
    /* List header used to link this structure to the "struct epitem" */
    // 用于将该结构链接到 epitem 结构体的列表头
    struct list_head llink;
 
    /* The "base" pointer is set to the container "struct epitem" */
    struct epitem *base;
 
    /*
     * Wait queue item that will be linked to the target file wait queue head.
     */
    wait_queue_entry_t wait;
 
    /* The wait queue head that linked the "wait" wait queue item */
    wait_queue_head_t *whead;
};
```



/usr/include/sys/epoll.h

```c
```



fs/eventpoll.c

```c
...
    
SYSCALL_DEFINE1(epoll_create1, int, flags)
{
	return do_epoll_create(flags);
}

/*
 * Open an eventpoll file descriptor.
 */
static int do_epoll_create(int flags)
{
	int error, fd;
	struct eventpoll *ep = NULL;
	struct file *file;

	/* Check the EPOLL_* constant for consistency.  */
	BUILD_BUG_ON(EPOLL_CLOEXEC != O_CLOEXEC);

    // 对传入的 flags 参数做简单的验证
	if (flags & ~EPOLL_CLOEXEC)
		return -EINVAL;
	/*
	 * Create the internal data structure ("struct eventpoll").
	 */
    // 内核申请分配 eventpoll 需要的内存空间
	error = ep_alloc(&ep);
	if (error < 0)
		return error;
	/*
	 * Creates all the items needed to setup an eventpoll file. That is,
	 * a file structure and a free file descriptor.
	 */
    // 为 epoll 实例分配了匿名文件和文件描述字，其中 fd 是文件描述字，file 是一个匿名文件
	fd = get_unused_fd_flags(O_RDWR | (flags & O_CLOEXEC));
	if (fd < 0) {
		error = fd;
		goto out_free_ep;
	}
    // eventpoll 的实例会保存一份匿名文件的引用，通过调用 fd_install 函数将匿名文件和文件描述字完成了绑定
    // anon_inode_getfile 将 eventpoll 作为匿名文件 file 的 private_data 保存了起来，这样，在之后通过 epoll 实例的文件描述字来查找时，就可以快速地定位到 eventpoll 对象
	file = anon_inode_getfile("[eventpoll]", &eventpoll_fops, ep,
				 O_RDWR | (flags & O_CLOEXEC));
	if (IS_ERR(file)) {
		error = PTR_ERR(file);
		goto out_free_fd;
	}
	ep->file = file;
	fd_install(fd, file);
    // 将文件描述字作为 epoll 的文件句柄，被返回给 epoll_create 的调用者
	return fd;

out_free_fd:
	put_unused_fd(fd);
out_free_ep:
	ep_free(ep);
	return error;
}


static int ep_alloc(struct eventpoll **pep)
{
	int error;
	struct user_struct *user;
	struct eventpoll *ep;

	user = get_current_user();
	error = -ENOMEM;
	ep = kzalloc(sizeof(*ep), GFP_KERNEL);
	if (unlikely(!ep))
		goto free_uid;

	mutex_init(&ep->mtx);
	rwlock_init(&ep->lock);
	init_waitqueue_head(&ep->wq);
	init_waitqueue_head(&ep->poll_wait);
	INIT_LIST_HEAD(&ep->rdllist);
	ep->rbr = RB_ROOT_CACHED;
	ep->ovflist = EP_UNACTIVE_PTR;
	ep->user = user;

	*pep = ep;

	return 0;

free_uid:
	free_uid(user);
	return error;
}
```





**epoll 的惊群问题:**





**样例：**

```c
#include <sys/epoll.h>

#define MAXEVENTS 128
 
char rot13_char(char c) {
    if ((c >= 'a' && c <= 'm') || (c >= 'A' && c <= 'M'))
        return c + 13;
    else if ((c >= 'n' && c <= 'z') || (c >= 'N' && c <= 'Z'))
        return c - 13;
    else
        return c;
}
 
void make_nonblocking(int fd) {
    fcntl(fd, F_SETFL, O_NONBLOCK);
}

int tcp_nonblocking_server_listen(int port) {
    int listenfd;
    listenfd = socket(AF_INET, SOCK_STREAM, 0);

    make_nonblocking(listenfd);

    struct sockaddr_in server_addr;
    bzero(&server_addr, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    server_addr.sin_port = htons(port);

    int on = 1;
    setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &on, sizeof(on));

    int rt1 = bind(listenfd, (struct sockaddr *) &server_addr, sizeof(server_addr));
    if (rt1 < 0) {
        error(1, errno, "bind failed ");
    }

    int rt2 = listen(listenfd, LISTENQ);
    if (rt2 < 0) {
        error(1, errno, "listen failed ");
    }

    signal(SIGPIPE, SIG_IGN);

    return listenfd;
}


int main(int argc, char **argv) {
    int listen_fd, socket_fd;
    int n, i;
    int efd;
    struct epoll_event event;
    struct epoll_event *events;
 
    listen_fd = tcp_nonblocking_server_listen(SERV_PORT);
 
    efd = epoll_create1(0);
    if (efd == -1) {
        error(1, errno, "epoll create failed");
    }
 
    event.data.fd = listen_fd;
    // 边缘触发
    event.events = EPOLLIN | EPOLLET;
    if (epoll_ctl(efd, EPOLL_CTL_ADD, listen_fd, &event) == -1) {
        error(1, errno, "epoll_ctl add listen fd failed");
    }
 
    /* Buffer where events are returned */
    events = calloc(MAXEVENTS, sizeof(event));
 
    while (1) {
        n = epoll_wait(efd, events, MAXEVENTS, -1);
        printf("epoll_wait wakeup\n");
        for (i = 0; i < n; i++) {
            if ((events[i].events & EPOLLERR) ||
                (events[i].events & EPOLLHUP) ||
                (!(events[i].events & EPOLLIN))) {
                fprintf(stderr, "epoll error\n");
                close(events[i].data.fd);
                continue;
            } else if (listen_fd == events[i].data.fd) {
                struct sockaddr_storage ss;
                socklen_t slen = sizeof(ss);
                int fd = accept(listen_fd, (struct sockaddr *) &ss, &slen);
                if (fd < 0) {
                    error(1, errno, "accept failed");
                } else {
                    make_nonblocking(fd);
                    event.data.fd = fd;
                    event.events = EPOLLIN | EPOLLET; //edge-triggered
                    if (epoll_ctl(efd, EPOLL_CTL_ADD, fd, &event) == -1) {
                        error(1, errno, "epoll_ctl add connection fd failed");
                    }
                }
                continue;
            } else {
                socket_fd = events[i].data.fd;
                printf("get event on socket fd == %d \n", socket_fd);
                while (1) {
                    char buf[512];
                    if ((n = read(socket_fd, buf, sizeof(buf))) < 0) {
                        if (errno != EAGAIN) {
                            error(1, errno, "read error");
                            close(socket_fd);
                        }
                        break;
                    } else if (n == 0) {
                        close(socket_fd);
                        break;
                    } else {
                        for (i = 0; i < n; ++i) {
                            buf[i] = rot13_char(buf[i]);
                        }
                        if (write(socket_fd, buf, n) < 0) {
                            error(1, errno, "write error");
                        }
                    }
                }
            }
        }
    }
 
    free(events);
    close(listen_fd);
}
```





```c
#include <stdio.h>
#include <sys/socket.h>
#include <sys/epoll.h>
#include <arpa/inet.h>
#include <fcntl.h>
#include <unistd.h>
#include <errno.h>
#include <string.h>
#include <stdlib.h>
#include <time.h>

#define MAX_EVENTS 1024 /*监听上限*/
#define BUFLEN  4096    /*缓存区大小*/
#define SERV_PORT 6666  /*端口号*/

void recvdata(int fd,int events,void *arg);
void senddata(int fd,int events,void *arg);

/*描述就绪文件描述符的相关信息*/
struct myevent_s
{
    int fd;             //要监听的文件描述符
    int events;         //对应的监听事件，EPOLLIN和EPLLOUT
    void *arg;          //指向自己结构体指针
    void (*call_back)(int fd,int events,void *arg); //回调函数
    int status;         //是否在监听:1->在红黑树上(监听), 0->不在(不监听)
    char buf[BUFLEN];   
    int len;
    long last_active;   //记录每次加入红黑树 g_efd 的时间值
};

int g_efd;      //全局变量，作为红黑树根
struct myevent_s g_events[MAX_EVENTS+1];    //自定义结构体类型数组. +1-->listen fd

/*
 * 封装一个自定义事件，包括fd，这个fd的回调函数，还有一个额外的参数项
 * 注意：在封装这个事件的时候，为这个事件指明了回调函数，一般来说，一个fd只对一个特定的事件
 * 感兴趣，当这个事件发生的时候，就调用这个回调函数
 */
void eventset(struct myevent_s *ev, int fd, void (*call_back)(int fd,int events,void *arg), void *arg)
{
    ev->fd = fd;
    ev->call_back = call_back;
    ev->events = 0;
    ev->arg = arg;
    ev->status = 0;
    if(ev->len <= 0)
    {
        memset(ev->buf, 0, sizeof(ev->buf));
        ev->len = 0;
    }
    ev->last_active = time(NULL); //调用eventset函数的时间
    return;
}

/* 向 epoll监听的红黑树 添加一个文件描述符 */
void eventadd(int efd, int events, struct myevent_s *ev)
{
    struct epoll_event epv={0, {0}};
    int op = 0;
    epv.data.ptr = ev; // ptr指向一个结构体（之前的epoll模型红黑树上挂载的是文件描述符cfd和lfd，现在是ptr指针）
    epv.events = ev->events = events; //EPOLLIN 或 EPOLLOUT
    if(ev->status == 0)       //status 说明文件描述符是否在红黑树上 0不在，1 在
    {
        op = EPOLL_CTL_ADD; //将其加入红黑树 g_efd, 并将status置1
        ev->status = 1;
    }
    if(epoll_ctl(efd, op, ev->fd, &epv) < 0) // 添加一个节点
        printf("event add failed [fd=%d],events[%d]\n", ev->fd, events);
    else
        printf("event add OK [fd=%d],events[%0X]\n", ev->fd, events);
    return;
}

/* 从epoll 监听的 红黑树中删除一个文件描述符*/
void eventdel(int efd,struct myevent_s *ev)
{
    struct epoll_event epv = {0, {0}};
    if(ev->status != 1) //如果fd没有添加到监听树上，就不用删除，直接返回
        return;
    epv.data.ptr = NULL;
    ev->status = 0;
    epoll_ctl(efd, EPOLL_CTL_DEL, ev->fd, &epv);
    return;
}

/*  当有文件描述符就绪, epoll返回, 调用该函数与客户端建立链接 */
void acceptconn(int lfd,int events,void *arg)
{
    struct sockaddr_in cin;
    socklen_t len = sizeof(cin);
    int cfd, i;
    if((cfd = accept(lfd, (struct sockaddr *)&cin, &len)) == -1)
    {
        if(errno != EAGAIN && errno != EINTR)
        {
            sleep(1);
        }
        printf("%s:accept,%s\n",__func__, strerror(errno));
        return;
    }
    do
    {
        for(i = 0; i < MAX_EVENTS; i++) //从全局数组g_events中找一个空闲元素，类似于select中找值为-1的元素
        {
            if(g_events[i].status ==0)
                break;
        }
        if(i == MAX_EVENTS) // 超出连接数上限
        {
            printf("%s: max connect limit[%d]\n", __func__, MAX_EVENTS);
            break;
        }
        int flag = 0;
        if((flag = fcntl(cfd, F_SETFL, O_NONBLOCK)) < 0) //将cfd也设置为非阻塞
        {
            printf("%s: fcntl nonblocking failed, %s\n", __func__, strerror(errno));
            break;
        }
        eventset(&g_events[i], cfd, recvdata, &g_events[i]); //找到合适的节点之后，将其添加到监听树中，并监听读事件
        eventadd(g_efd, EPOLLIN, &g_events[i]);
    }while(0);

    printf("new connect[%s:%d],[time:%ld],pos[%d]",inet_ntoa(cin.sin_addr), ntohs(cin.sin_port), g_events[i].last_active, i);
    return;
}

/*读取客户端发过来的数据的函数*/
void recvdata(int fd, int events, void *arg)
{
    struct myevent_s *ev = (struct myevent_s *)arg;
    int len;

    len = recv(fd, ev->buf, sizeof(ev->buf), 0);    //读取客户端发过来的数据

    eventdel(g_efd, ev);                            //将该节点从红黑树上摘除

    if (len > 0) 
    {
        ev->len = len;
        ev->buf[len] = '\0';                        //手动添加字符串结束标记
        printf("C[%d]:%s\n", fd, ev->buf);                  

        eventset(ev, fd, senddata, ev);             //设置该fd对应的回调函数为senddata    
        eventadd(g_efd, EPOLLOUT, ev);              //将fd加入红黑树g_efd中,监听其写事件    

    } 
    else if (len == 0) 
    {
        close(ev->fd);
        /* ev-g_events 地址相减得到偏移元素位置 */
        printf("[fd=%d] pos[%ld], closed\n", fd, ev-g_events);
    } 
    else 
    {
        close(ev->fd);
        printf("recv[fd=%d] error[%d]:%s\n", fd, errno, strerror(errno));
    }   
    return;
}

/*发送给客户端数据*/
void senddata(int fd, int events, void *arg)
{
    struct myevent_s *ev = (struct myevent_s *)arg;
    int len;

    len = send(fd, ev->buf, ev->len, 0);    //直接将数据回射给客户端

    eventdel(g_efd, ev);                    //从红黑树g_efd中移除

    if (len > 0) 
    {
        printf("send[fd=%d], [%d]%s\n", fd, len, ev->buf);
        eventset(ev, fd, recvdata, ev);     //将该fd的回调函数改为recvdata
        eventadd(g_efd, EPOLLIN, ev);       //重新添加到红黑树上，设为监听读事件
    }
    else 
    {
        close(ev->fd);                      //关闭链接
        printf("send[fd=%d] error %s\n", fd, strerror(errno));
    }
    return ;
}

/*创建 socket, 初始化lfd */
void initlistensocket(int efd, short port)
{
    struct sockaddr_in sin;

    int lfd = socket(AF_INET, SOCK_STREAM, 0);
    fcntl(lfd, F_SETFL, O_NONBLOCK);                //将socket设为非阻塞

    memset(&sin, 0, sizeof(sin));               //bzero(&sin, sizeof(sin))
    sin.sin_family = AF_INET;
    sin.sin_addr.s_addr = INADDR_ANY;
    sin.sin_port = htons(port);

    bind(lfd, (struct sockaddr *)&sin, sizeof(sin));

    listen(lfd, 20);

    /* void eventset(struct myevent_s *ev, int fd, void (*call_back)(int, int, void *), void *arg);  */
    eventset(&g_events[MAX_EVENTS], lfd, acceptconn, &g_events[MAX_EVENTS]);    

    /* void eventadd(int efd, int events, struct myevent_s *ev) */
    eventadd(efd, EPOLLIN, &g_events[MAX_EVENTS]);  //将lfd添加到监听树上，监听读事件

    return;
}

int main()
{
    int port=SERV_PORT;

    g_efd = epoll_create(MAX_EVENTS + 1); //创建红黑树,返回给全局 g_efd
    if(g_efd <= 0)
        printf("create efd in %s err %s\n", __func__, strerror(errno));

    initlistensocket(g_efd, port); //初始化监听socket

    struct epoll_event events[MAX_EVENTS + 1];  //定义这个结构体数组，用来接收epoll_wait传出的满足监听事件的fd结构体
    printf("server running:port[%d]\n", port);

    int checkpos = 0;
    int i;
    while(1)
    {
    /*    long now = time(NULL);
        for(i=0; i < 100; i++, checkpos++)
        {
            if(checkpos == MAX_EVENTS);
                checkpos = 0;
            if(g_events[checkpos].status != 1)
                continue;
            long duration = now -g_events[checkpos].last_active;
            if(duration >= 60)
            {
                close(g_events[checkpos].fd);
                printf("[fd=%d] timeout\n", g_events[checkpos].fd);
                eventdel(g_efd, &g_events[checkpos]);
            }
        } */
        //调用eppoll_wait等待接入的客户端事件,epoll_wait传出的是满足监听条件的那些fd的struct epoll_event类型
        int nfd = epoll_wait(g_efd, events, MAX_EVENTS+1, 1000);
        if (nfd < 0)
        {
            printf("epoll_wait error, exit\n");
            exit(-1);
        }
        for(i = 0; i < nfd; i++)
        {
            //evtAdd()函数中，添加到监听树中监听事件的时候将myevents_t结构体类型给了ptr指针
            //这里epoll_wait返回的时候，同样会返回对应fd的myevents_t类型的指针
            struct myevent_s *ev = (struct myevent_s *)events[i].data.ptr;
            //如果监听的是读事件，并返回的是读事件
            if((events[i].events & EPOLLIN) &&(ev->events & EPOLLIN))
            {
                ev->call_back(ev->fd, events[i].events, ev->arg);
            }
            //如果监听的是写事件，并返回的是写事件
            if((events[i].events & EPOLLOUT) && (ev->events & EPOLLOUT))
            {
                ev->call_back(ev->fd, events[i].events, ev->arg);
            }
        }
    }
    return 0;
}
```










# 通信模式

## 阻塞模式 
​	网络编程模型都是阻塞式的。所谓阻塞式，就是调用发起后不会直接返回，由操作系统内核处理之后才会返回。 



## 非阻塞模式










# 设置 socket 套接字
## setsocketopt 函数







# 父子进程：

```c
pid_t fork(void)
返回：在子进程中为 0，在父进程中为子进程 ID，若出错则为 -1
```

​	fork 函数实现的时候会把当前父进程的所有相关值都克隆一份，包括地址空间、打开的文件描述符、程序计数器等，就连执行代码也会拷贝一份. 



## 僵尸进程：

​	当一个**子进程**退出时，系统内核还保留了该进程的若干信息，比如退出状态。这样的进程如果**不回收**，就会变成僵尸进程。"僵尸"进程会被挂到进程号为 1 的 init 进程上, 僵尸进程会占用不必要的内存空间.

​	

### 进程回收资源：

​	回收资源，分别是调用 wait 和 waitpid 函数

```c
pid_t wait(int *statloc);
pid_t waitpid(pid_t pid, int *statloc, int options);
```

​	函数 wait 和 waitpid 都可以返回两个值，一个是函数返回值，表示已终止子进程的进程 ID 号，另一个则是通过 statloc 指针返回子进程终止的实际状态。这个状态可能的值为正常终止、被信号杀死、作业控制停止等.

-   waitpid: 即使还有未终止的子进程也不要阻塞在 waitpid 上

-   wait: 函数在有未终止子进程的情况下，没有办法不阻塞



### 信号注册：

​	处理子进程退出的方式一般是注册一个信号处理函数，捕捉信号 SIGCHILD 信号，然后再在信号处理函数里调用 waitpid 函数来完成子进程资源的回收。SIGCHLD 是子进程退出或者中断时由内核向父进程发出的信号，默认这个信号是忽略的。所以，如果想在子进程退出时能回收它，需要像下面一样，注册一个 SIGCHOLD 函数。

```c
signal(SIGCHLD, sigchld_handler);　　
```



​	假设有两个客户端，服务器初始监听在套接字 lisnted_fd 上。当第一个客户端发起连接请求，连接建立后产生出连接套接字，此时，父进程派生出一个子进程，在子进程中，使用连接套接字和客户端通信，因此子进程不需要关心监听套接字，只需要关心连接套接字；父进程则相反，将客户服务交给子进程来处理，因此父进程不需要关心连接套接字，只需要关心监听套接字。

```c
#define SERV_PORT	43211
#define	MAX_LINE	4096
 
char rot13_char(char c) {
    if ((c >= 'a' && c <= 'm') || (c >= 'A' && c <= 'M'))
        return c + 13;
    else if ((c >= 'n' && c <= 'z') || (c >= 'N' && c <= 'Z'))
        return c - 13;
    else
        return c;
}
 
void child_run(int fd) {
    char outbuf[MAX_LINE + 1];
    size_t outbuf_used = 0;
    ssize_t result;
 
    while (1) {
        char ch;
        result = recv(fd, &ch, 1, 0);
        if (result == 0) {
            break;
        } else if (result == -1) {
            perror("read");
            break;
        }
 
        if (outbuf_used < sizeof(outbuf)) {
            outbuf[outbuf_used++] = rot13_char(ch);
        }
 
        if (ch == '\n') {
            send(fd, outbuf, outbuf_used, 0);
            outbuf_used = 0;
            continue;
        }
    }
}
 
void sigchld_handler(int sig) {
    // 选项 WNOHANG 用来告诉内核，即使还有未终止的子进程也不要阻塞在 waitpid 上。注意这里不可以使用 wait，因为 wait 函数在有未终止子进程的情况下，没有办法不阻塞
    while (waitpid(-1, 0, WNOHANG) > 0);
    return;
}
 
int tcp_server_listen(int port) {
    int listenfd;
    listenfd = socket(AF_INET, SOCK_STREAM, 0);

    struct sockaddr_in server_addr;
    bzero(&server_addr, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    server_addr.sin_port = htons(port);

    int on = 1;
    setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &on, sizeof(on));

    int rt1 = bind(listenfd, (struct sockaddr *) &server_addr, sizeof(server_addr));
    if (rt1 < 0) {
        error(1, errno, "bind failed ");
    }

    int rt2 = listen(listenfd, LISTENQ);
    if (rt2 < 0) {
        error(1, errno, "listen failed ");
    }

    signal(SIGPIPE, SIG_IGN);

    return listenfd;
}

int main(int c, char **v) {
    int listener_fd = tcp_server_listen(SERV_PORT);
    // 注册信号处理函数，用来回收子进程资源
    signal(SIGCHLD, sigchld_handler);
    while (1) {
        struct sockaddr_storage ss;
        socklen_t slen = sizeof(ss);
        int fd = accept(listener_fd, (struct sockaddr *) &ss, &slen);
        if (fd < 0) {
            error(1, errno, "accept failed");
            exit(1);
        }
 
        /*
        	从父进程派生出的子进程，同时也会复制一份描述字，也就是说，连接套接字和监听套接字的引用计数都会被加 1，
        	而调用 close 函数则会对引用计数进行减 1 操作，这样在套接字引用计数到 0 时，才可以将套接字资源回收
        */ 
        if (fork() == 0) {
            // 子进程不需要关心监听套接字，故而在这里关闭掉监听套接字 listen_fd，之后调用 child_run 函数使用已连接套接字 fd 来进行数据读写
            close(listener_fd);
            child_run(fd);
            exit(0);
        } else {
            // 父进程不需要关心连接套接字，所以在这里关闭连接套接字
            close(fd);
        }
    }
 
    return 0;
}
```







​	



## 孤儿进程：







# POSIX 线程模型:

1.   **创建线程**

     ```c
     int pthread_create(pthread_t *tid, const pthread_attr_t *attr, void *(*func)(void *), void *arg);
      
     返回：若成功则为 0，若出错则为正的 Exxx 值
     ```

     -   线程 ID（tid）其数据类型为 pthread_t，一般是 unsigned int。pthread_create 函数的第一个输出参数 tid 就是代表了线程 ID，如果创建线程成功，tid 就返回正确的线程 ID。
     -   pthread_attr_t 来描述线程属性 (比如优先级、是否应该成为一个守护进程等)，一般不会特殊设置，可以直接指定这个参数为 NULL。
     -   第三个参数为新线程的入口函数，该函数可以接收一个参数 arg，类型为指针. 如果想给**线程入口函数传多个值，那么需要把这些值包装成一个结构体**，再把这个结构体的地址作为 pthread_create 的第四个参数，在线程入口函数内，再将该地址转为该结构体的指针对象。

2.   **获取线程id**

     ```c
     // 返回线程 tid
     pthread_t pthread_self(void)
     ```
     
3.   **终止线程**

     ```c
     void pthread_exit(void *status)
     ```

     在父线程内调用, 当调用这个函数之后，父线程会等待其他所有的子线程终止，之后父线程自己终止. 

     ```c
     // 可以指定某个子线程终止
     int pthread_cancel(pthread_t tid)
     ```

4.   **回收已终止线程的资源**

     ```c
     int pthread_join(pthread_t tid, void ** thread_return)
     ```

     当调用 pthread_join 时，主线程会阻塞，直到对应 tid 的子线程自然终止。和 pthread_cancel 不同的是，它不会强迫子线程终止。

5.   **分离线程**

     ```c
     int pthread_detach(pthread_t tid)
     ```

     线程可以在入口函数开始的地方，把自己设置为分离的，这样就能在它终止后自动回收相关的线程资源了，就不需要调用 **pthread_join** 函数了



**测试用例：**

```c

char rot13_char(char c) {
    if ((c >= 'a' && c <= 'm') || (c >= 'A' && c <= 'M'))
        return c + 13;
    else if ((c >= 'n' && c <= 'z') || (c >= 'N' && c <= 'Z'))
        return c - 13;
    else
        return c;
}

void loop_echo(int fd) {
    char outbuf[MAX_LINE + 1];
    size_t outbuf_used = 0;
    ssize_t result;
    while (1) {
        char ch;
        result = recv(fd, &ch, 1, 0);

        //断开连接或者出错
        if (result == 0) {
            break;
        } else if (result == -1) {
            error(1, errno, "read error");
            break;
        }

        if (outbuf_used < sizeof(outbuf)) {
            outbuf[outbuf_used++] = rot13_char(ch);
        }

        if (ch == '\n') {
            send(fd, outbuf, outbuf_used, 0);
            outbuf_used = 0;
            continue;
        }
    }
}


int tcp_server_listen(int port) {
    int listenfd;
    listenfd = socket(AF_INET, SOCK_STREAM, 0);

    struct sockaddr_in server_addr;
    bzero(&server_addr, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    server_addr.sin_port = htons(port);

    int on = 1;
    setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &on, sizeof(on));

    int rt1 = bind(listenfd, (struct sockaddr *) &server_addr, sizeof(server_addr));
    if (rt1 < 0) {
        error(1, errno, "bind failed ");
    }

    int rt2 = listen(listenfd, LISTENQ);
    if (rt2 < 0) {
        error(1, errno, "listen failed ");
    }

    signal(SIGPIPE, SIG_IGN);

    return listenfd;
}


void thread_run(void *arg) {
    // 将子线程转变为分离的，也就意味着子线程独自负责线程资源回收
    pthread_detach(pthread_self());
    int fd = (int) arg;
    loop_echo(fd);
}
 
int main(int c, char **v) {
    int listener_fd = tcp_server_listen(SERV_PORT);
    pthread_t tid;
    
    while (1) {
        struct sockaddr_storage ss;
        socklen_t slen = sizeof(ss);
        int fd = accept(listener_fd, (struct sockaddr *) &ss, &slen);
        if (fd < 0) {
            error(1, errno, "accept failed");
        } else {
            pthread_create(&tid, NULL, &thread_run, (void *) fd);
        }
    }
 
    return 0;
}
```



优化（利用线程池）：

```c
// 定义一个队列
typedef struct {
    int number;  // 队列里的描述字最大个数
    int *fd;     // 这是一个数组指针
    int front;   // 当前队列的头位置
    int rear;    // 当前队列的尾位置
    pthread_mutex_t mutex;  // 锁
    pthread_cond_t cond;    // 条件变量
} block_queue;
 
// 初始化队列
void block_queue_init(block_queue *blockQueue, int number) {
    blockQueue->number = number;
    blockQueue->fd = calloc(number, sizeof(int));
    blockQueue->front = blockQueue->rear = 0;
    pthread_mutex_init(&blockQueue->mutex, NULL);
    pthread_cond_init(&blockQueue->cond, NULL);
}
 
// 往队列里放置一个描述字 fd
void block_queue_push(block_queue *blockQueue, int fd) {
    // 一定要先加锁，因为有多个线程需要读写队列
    pthread_mutex_lock(&blockQueue->mutex);
    // 将描述字放到队列尾的位置
    blockQueue->fd[blockQueue->rear] = fd;
    // 如果已经到最后，重置尾的位置
    if (++blockQueue->rear == blockQueue->number) {
        blockQueue->rear = 0;
    }
    printf("push fd %d", fd);
    // 通知其他等待读的线程，有新的连接字等待处理
    pthread_cond_signal(&blockQueue->cond);
    // 解锁
    pthread_mutex_unlock(&blockQueue->mutex);
}
 
// 从队列里读出描述字进行处理
int block_queue_pop(block_queue *blockQueue) {
    // 加锁
    pthread_mutex_lock(&blockQueue->mutex);
    // 判断队列里没有新的连接字可以处理，就一直条件等待，直到有新的连接字入队列
    while (blockQueue->front == blockQueue->rear)
        pthread_cond_wait(&blockQueue->cond, &blockQueue->mutex);
    // 取出队列头的连接字
    int fd = blockQueue->fd[blockQueue->front];
    // 如果已经到最后，重置头的位置
    if (++blockQueue->front == blockQueue->number) {
        blockQueue->front = 0;
    }
    printf("pop fd %d", fd);
    // 解锁
    pthread_mutex_unlock(&blockQueue->mutex);
    // 返回连接字
    return fd;
}  



void thread_run(void *arg) {
    pthread_t tid = pthread_self();
    pthread_detach(tid);
 
    block_queue *blockQueue = (block_queue *) arg;
    while (1) {
        int fd = block_queue_pop(blockQueue);
        printf("get fd in thread, fd==%d, tid == %d", fd, tid);
        loop_echo(fd);
    }
}
 
int main(int c, char **v) {
    int listener_fd = tcp_server_listen(SERV_PORT);
 
    block_queue blockQueue;
    block_queue_init(&blockQueue, BLOCK_QUEUE_SIZE);
 
    // 预创建了多个线程，组成了一个线程池
    thread_array = calloc(THREAD_NUMBER, sizeof(Thread));
    int i;
    for (i = 0; i < THREAD_NUMBER; i++) {
        pthread_create(&(thread_array[i].thread_tid), NULL, &thread_run, (void *) &blockQueue);
    }
 
    while (1) {
        struct sockaddr_storage ss;
        socklen_t slen = sizeof(ss);
        // 在新连接建立后，将连接描述字加入到队列中
        int fd = accept(listener_fd, (struct sockaddr *) &ss, &slen);
        if (fd < 0) {
            error(1, errno, "accept failed");
        } else {
            block_queue_push(&blockQueue, fd);
        }
    }
 
    return 0;
}
```






























































































